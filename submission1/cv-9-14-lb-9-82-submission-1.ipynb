{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-12-05T04:01:52.051047Z",
     "iopub.status.busy": "2020-12-05T04:01:52.050318Z",
     "iopub.status.idle": "2020-12-05T04:01:53.230894Z",
     "shell.execute_reply": "2020-12-05T04:01:53.230114Z"
    },
    "papermill": {
     "duration": 1.201907,
     "end_time": "2020-12-05T04:01:53.231020",
     "exception": false,
     "start_time": "2020-12-05T04:01:52.029113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/msbd5001-fall2020/sampleSubmission.csv\n",
      "/kaggle/input/msbd5001-fall2020/train.csv\n",
      "/kaggle/input/msbd5001-fall2020/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import lightgbm as lgb \n",
    "import catboost as ctb\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T04:01:53.284071Z",
     "iopub.status.busy": "2020-12-05T04:01:53.281935Z",
     "iopub.status.idle": "2020-12-05T04:01:59.634946Z",
     "shell.execute_reply": "2020-12-05T04:01:59.634291Z"
    },
    "papermill": {
     "duration": 6.388034,
     "end_time": "2020-12-05T04:01:59.635047",
     "exception": false,
     "start_time": "2020-12-05T04:01:53.247013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.3.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.3.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import gc\n",
    "from math import pi\n",
    "from math import cos\n",
    "from math import floor\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from typing import Callable, List\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, BayesianRidge, Lasso, HuberRegressor, ElasticNet, Lars\n",
    "from sklearn.svm import LinearSVR, SVR, NuSVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor, BaggingRegressor\n",
    "\n",
    "\n",
    "class CosineAnnealingLearningRateSchedule(tf.keras.callbacks.Callback):\n",
    "\t# constructor\n",
    "\tdef __init__(self, n_epochs, n_cycles, lrate_max, verbose=0):\n",
    "\t\tself.epochs = n_epochs\n",
    "\t\tself.cycles = n_cycles\n",
    "\t\tself.lr_max = lrate_max\n",
    "\t\tself.lrates = list()\n",
    " \n",
    "\t# calculate learning rate for an epoch\n",
    "\tdef cosine_annealing(self, epoch, n_epochs, n_cycles, lrate_max):\n",
    "\t\tepochs_per_cycle = floor(n_epochs/n_cycles)\n",
    "\t\tcos_inner = (pi * (epoch % epochs_per_cycle)) / (epochs_per_cycle)\n",
    "\t\treturn lrate_max/2 * (cos(cos_inner) + 1)\n",
    " \n",
    "\t# calculate and set learning rate at the start of the epoch\n",
    "\tdef on_epoch_begin(self, epoch, logs=None):\n",
    "\t\t# calculate learning rate\n",
    "\t\tlr = self.cosine_annealing(epoch, self.epochs, self.cycles, self.lr_max)\n",
    "\t\t# set learning rate\n",
    "\t\ttf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "\t\t# log value\n",
    "\t\tself.lrates.append(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T04:01:59.677539Z",
     "iopub.status.busy": "2020-12-05T04:01:59.675741Z",
     "iopub.status.idle": "2020-12-05T04:01:59.724222Z",
     "shell.execute_reply": "2020-12-05T04:01:59.723615Z"
    },
    "papermill": {
     "duration": 0.07369,
     "end_time": "2020-12-05T04:01:59.724324",
     "exception": false,
     "start_time": "2020-12-05T04:01:59.650634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "holidays =[\n",
    "    \"2017-1-1\",\n",
    "    \"2017-1-2\",\n",
    "    \"2017-1-28\",\n",
    "    \"2017-1-30\",\n",
    "    \"2017-1-31\",\n",
    "    \"2017-4-4\",\n",
    "    \"2017-4-14\",\n",
    "    \"2017-4-15\",\n",
    "    \"2017-4-17\",\n",
    "    \"2017-5-1\",\n",
    "    \"2017-5-3\",\n",
    "    \"2017-5-30\",\n",
    "    \"2017-7-1\",\n",
    "    \"2017-10-1\",\n",
    "    \"2017-10-2\",\n",
    "    \"2017-10-5\",\n",
    "    \"2017-10-28\",\n",
    "    \"2017-12-25\",\n",
    "    \"2017-12-26\",\n",
    "    \"2018-1-1\",\n",
    "    \"2018-2-16\",\n",
    "    \"2018-2-17\",\n",
    "    \"2018-2-19\",\n",
    "    \"2018-3-30\",\n",
    "    \"2018-3-31\",\n",
    "    \"2018-4-2\",\n",
    "    \"2018-4-5\",\n",
    "    \"2018-5-1\",\n",
    "    \"2018-5-22\",\n",
    "    \"2018-6-18\",\n",
    "    \"2018-7-1\",\n",
    "    \"2018-7-2\",\n",
    "    \"2018-9-24\",\n",
    "    \"2018-9-25\",\n",
    "    \"2018-10-1\",\n",
    "    \"2018-10-17\",\n",
    "    \"2018-12-25\",\n",
    "    \"2018-12-26\",\n",
    "]\n",
    "holidays = [pd.to_datetime(h, format='%Y-%m-%d').tz_localize(\"HongKong\") for h in holidays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T04:01:59.759654Z",
     "iopub.status.busy": "2020-12-05T04:01:59.758980Z",
     "iopub.status.idle": "2020-12-05T04:01:59.762262Z",
     "shell.execute_reply": "2020-12-05T04:01:59.761643Z"
    },
    "papermill": {
     "duration": 0.022529,
     "end_time": "2020-12-05T04:01:59.762397",
     "exception": false,
     "start_time": "2020-12-05T04:01:59.739868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "random_state = 2020\n",
    "# num_fold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T04:01:59.799662Z",
     "iopub.status.busy": "2020-12-05T04:01:59.799003Z",
     "iopub.status.idle": "2020-12-05T04:01:59.802218Z",
     "shell.execute_reply": "2020-12-05T04:01:59.801597Z"
    },
    "papermill": {
     "duration": 0.024817,
     "end_time": "2020-12-05T04:01:59.802322",
     "exception": false,
     "start_time": "2020-12-05T04:01:59.777505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_files():\n",
    "    sample_submission_df = pd.read_csv(\"/kaggle/input/msbd5001-fall2020/sampleSubmission.csv\")\n",
    "    sample_submission_df.shape\n",
    "\n",
    "    train_df = pd.read_csv(\"/kaggle/input/msbd5001-fall2020/train.csv\")\n",
    "    test_df = pd.read_csv(\"/kaggle/input/msbd5001-fall2020/test.csv\")\n",
    "    train_df.date = pd.to_datetime(train_df.date, format='%d/%m/%Y %H:%M').dt.tz_localize(\"HongKong\")\n",
    "    test_df.date = pd.to_datetime(test_df.date, format='%d/%m/%Y %H:%M').dt.tz_localize(\"HongKong\")\n",
    "    \n",
    "    test_df[\"is_test\"] = True\n",
    "    train_df[\"is_test\"] = False\n",
    "\n",
    "    all_df = pd.concat([train_df, test_df], ignore_index=True).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    return sample_submission_df, train_df, test_df, all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-12-05T04:01:59.845142Z",
     "iopub.status.busy": "2020-12-05T04:01:59.840111Z",
     "iopub.status.idle": "2020-12-05T04:01:59.847791Z",
     "shell.execute_reply": "2020-12-05T04:01:59.847258Z"
    },
    "papermill": {
     "duration": 0.030605,
     "end_time": "2020-12-05T04:01:59.847895",
     "exception": false,
     "start_time": "2020-12-05T04:01:59.817290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_missing_dates(train_df, test_df, all_df, hole_random_state):\n",
    "    all_df[\"lag_date\"] = all_df[\"date\"].shift()\n",
    "    all_df[\"interval\"] = (all_df[\"date\"] - all_df[\"lag_date\"])\n",
    "    train_df[\"lag_date\"] = train_df[\"date\"].shift()\n",
    "    train_df[\"interval\"] = (train_df[\"date\"] - train_df[\"lag_date\"])\n",
    "    \n",
    "    print(\"Missing intervals 2018\")\n",
    "    print(train_df[train_df.date.dt.year==2018].interval.value_counts())\n",
    "    \n",
    "    # all dates\n",
    "    ideal_date = pd.period_range(\n",
    "        start=\"2017-01-01 00:00:00\",\n",
    "        end=\"2018-12-31 23:00:00\",\n",
    "        freq=\"H\")\n",
    "    ideal_df = pd.DataFrame({\n",
    "        \"date\": ideal_date.to_timestamp(),\n",
    "    })\n",
    "    ideal_df[\"date\"] = ideal_df[\"date\"].dt.tz_localize(\"HongKong\")\n",
    "\n",
    "    full_df = pd.merge(ideal_df, all_df, on=\"date\", how=\"left\")\n",
    "    \n",
    "    full_df[\"lag_date\"] = full_df[\"date\"].shift()\n",
    "    full_df[\"interval\"] = (full_df[\"date\"] - full_df[\"lag_date\"])\n",
    "    full_df[\"interval\"].value_counts()\n",
    "    full_df = full_df[[\"date\", \"id\", \"speed\", \"is_test\"]]\n",
    "    \n",
    "    \n",
    "    full_df = full_df[~full_df.is_test.isna()]\n",
    "    full_df[\"id\"] = full_df[\"id\"].astype(int)\n",
    "    \n",
    "    full_2017_df = full_df[full_df.date.dt.year==2017].reset_index(drop=True)\n",
    "    full_2018_df = full_df[full_df.date.dt.year==2018].reset_index(drop=True)\n",
    "    full_2017_df.shape, full_2018_df.shape\n",
    "    \n",
    "    date_holes_2018 = full_2018_df[full_2018_df.speed.isna()].date\n",
    "    \n",
    "    # simulate missing intervals in 2017 data\n",
    "#     date_holes = full_2017_df.sample(frac=3504/full_2018_df.shape[0], random_state=hole_random_state).date\n",
    "    date_holes = full_2018_df[full_2018_df.speed.isna()].date.apply(lambda x: x.replace(year=2017))\n",
    "    \n",
    "    print(date_holes_2018)\n",
    "    print(date_holes)\n",
    "    \n",
    "    full_2017_df[\"truth\"] = full_2017_df.speed\n",
    "    full_2017_df.loc[full_2017_df.date.isin(date_holes), \"speed\"] = np.nan\n",
    "    full_2017_df[\"speed\"].isna().sum()\n",
    "    \n",
    "#     temp_df = full_2017_df[~full_2017_df.speed.isna()].reset_index(drop=True)\n",
    "#     temp_df[\"lag_date\"] = temp_df[\"date\"].shift()\n",
    "#     temp_df[\"interval\"] = (temp_df[\"date\"] - temp_df[\"lag_date\"])\n",
    "    \n",
    "#     print(\"Mocking Missing intervals 2017\")\n",
    "#     print(temp_df[temp_df.date.dt.year == 2017][\"interval\"].value_counts())\n",
    "    \n",
    "    full_2018_df[\"truth\"] = full_2018_df.speed\n",
    "    full_df = pd.concat([full_2017_df, full_2018_df], ignore_index=True)\n",
    "    return train_df, test_df, full_df, full_2017_df, full_2018_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T04:01:59.893738Z",
     "iopub.status.busy": "2020-12-05T04:01:59.888595Z",
     "iopub.status.idle": "2020-12-05T04:01:59.906592Z",
     "shell.execute_reply": "2020-12-05T04:01:59.905998Z"
    },
    "papermill": {
     "duration": 0.042759,
     "end_time": "2020-12-05T04:01:59.906689",
     "exception": false,
     "start_time": "2020-12-05T04:01:59.863930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def speed_feature_engineering(df: pd.DataFrame, speed_feature=\"speed\"):\n",
    "    \n",
    "    \n",
    "    # % change\n",
    "    for lag in [1, 2, 3]:\n",
    "        df[f\"Prev_{lag}_%_change\"] = df[\"per_change\"].shift() \n",
    "    \n",
    "    # Prev/Post hours speed compareing to their weekday_hour average\n",
    "    for lag in [1, 2, 3, 24]:\n",
    "        df[f\"Prev_{lag}_{speed_feature}\"] = df[speed_feature].shift(lag)\n",
    "        df[f\"Post_{lag}_{speed_feature}\"] = df[speed_feature].shift(-lag)\n",
    "           \n",
    "        df[f\"Prev_{lag}_weekday_hour_mean\"] = df[\"weekday_hour_avg_speed\"].shift(lag)\n",
    "        df[f\"Post_{lag}_weekday_hour_mean\"] = df[\"weekday_hour_avg_speed\"].shift(-lag)\n",
    "        \n",
    "        df[f\"Prev_{lag}_weekday_hour_std\"] = df[\"weekday_hour_std_speed\"].shift(lag)\n",
    "        df[f\"Post_{lag}_weekday_hour_std\"] = df[\"weekday_hour_std_speed\"].shift(-lag)\n",
    "        \n",
    "        \n",
    "        # Prev/Post speed compare to Prev/Post mean spped\n",
    "        df[f\"Ratio_Prev_{lag}_weekday_hour_mean\"] = df[f\"Prev_{lag}_{speed_feature}\"] / df[f\"Prev_{lag}_weekday_hour_mean\"]\n",
    "        df[f\"Ratio_Post_{lag}_weekday_hour_mean\"] = df[f\"Post_{lag}_{speed_feature}\"] / df[f\"Post_{lag}_weekday_hour_mean\"]\n",
    "        \n",
    "        df[f\"PDiff_Prev_{lag}_weekday_hour_mean\"] = (df[f\"Prev_{lag}_{speed_feature}\"] - df[f\"Prev_{lag}_weekday_hour_mean\"]) / df[f\"Prev_{lag}_weekday_hour_mean\"]\n",
    "        df[f\"PDiff_Post_{lag}_weekday_hour_mean\"] = (df[f\"Post_{lag}_{speed_feature}\"] - df[f\"Post_{lag}_weekday_hour_mean\"]) / df[f\"Post_{lag}_weekday_hour_mean\"]\n",
    "        \n",
    "        df[f\"Z_Score_Prev_{lag}_weekday_hour\"] = (df[f\"Prev_{lag}_{speed_feature}\"] - df[f\"Prev_{lag}_weekday_hour_mean\"]) / df[f\"Prev_{lag}_weekday_hour_std\"]\n",
    "        df[f\"Z_Score_Post_{lag}_weekday_hour\"] = (df[f\"Post_{lag}_{speed_feature}\"] - df[f\"Post_{lag}_weekday_hour_mean\"]) / df[f\"Post_{lag}_weekday_hour_std\"]\n",
    "    \n",
    "    # % change between prevs & posts\n",
    "    for lag in [3]:\n",
    "        df[f\"PDiff_Prev_1_{lag}\"] = (df[f\"Prev_{lag}_{speed_feature}\"] - df[f\"Prev_1_{speed_feature}\"]) / df[f\"Prev_{lag}_{speed_feature}\"]\n",
    "        \n",
    "\n",
    "    # Prev/Post Moving simple statistics\n",
    "    for win in [3, 5, 10]:\n",
    "        df[f\"Prev_{win}_Avg_{speed_feature}\"] = df[speed_feature].shift().rolling(win, win_type=None, min_periods=1).mean()\n",
    "        df[f\"Prev_{win}_Max_{speed_feature}\"] = df[speed_feature].shift().rolling(win, win_type=None, min_periods=1).max()\n",
    "        df[f\"Prev_{win}_Min_{speed_feature}\"] = df[speed_feature].shift().rolling(win, win_type=None, min_periods=1).min()\n",
    "        \n",
    "        df[f\"Post_{win}_Avg_{speed_feature}\"] = df[f\"Prev_{win}_Avg_{speed_feature}\"].shift(-win-1)\n",
    "        df[f\"Post_{win}_Max_{speed_feature}\"] = df[f\"Prev_{win}_Max_{speed_feature}\"].shift(-win-1)\n",
    "        df[f\"Post_{win}_Min_{speed_feature}\"] = df[f\"Prev_{win}_Min_{speed_feature}\"].shift(-win-1)\n",
    "    \n",
    "        df[f\"Ratio_Prev_2_{win}_Avg\"] = df[f\"Prev_2_{speed_feature}\"] / (df[f\"Prev_{win}_Avg_{speed_feature}\"])\n",
    "        \n",
    "        df[f\"Prev_{win}_EMA_{speed_feature}\"] = df[speed_feature].shift().ewm(win, min_periods=1, ignore_na=True).mean()\n",
    "    \n",
    "    # Prev/Post Moving groupby statistics\n",
    "#     for win in [3, 5]:\n",
    "#         df[f\"Prev_{win}_Avg_hourly_{speed_feature}\"] = full_df.groupby(\"weekday_hour\")[speed_feature].shift().rolling(win, min_periods=1).median()\n",
    "# #         df[f\"Post_{win}_Avg_hourly_{speed_feature}\"] = df[f\"Prev_{win}_Avg_hourly_{speed_feature}\"].shift(-win-1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def feature_engineering(full_df, fill_speed_func):\n",
    "    full_df[\"month\"] = full_df.date.dt.month\n",
    "    full_df[\"day\"] = full_df.date.dt.day\n",
    "    full_df[\"hour\"] = full_df.date.dt.hour # 24\n",
    "    full_df[\"weekday\"] = full_df.date.dt.weekday # 7\n",
    "    full_df[\"dayofyear\"] = full_df.date.dt.dayofyear\n",
    "    full_df[\"weekofyear\"] = full_df.date.dt.weekofyear\n",
    "\n",
    "    full_df[\"holiday\"] = full_df.date.dt.floor('d').isin(holidays)\n",
    "\n",
    "    # sun -> 6 \n",
    "    full_df.loc[full_df[\"weekday\"]==0, \"weekday\"] = 7\n",
    "    full_df[\"weekday\"] -= 1\n",
    "\n",
    "    full_df[\"weekday_hour\"] = full_df[\"weekday\"] * 24 + full_df[\"hour\"]\n",
    "    full_df[\"day_weekday_hour\"] = (full_df[\"day\"]//7) * (7*24) + full_df[\"weekday\"] * 24 + full_df[\"hour\"]\n",
    "    full_df[\"quarter\"] = full_df.date.dt.quarter\n",
    "    full_df[\"year_weekday_hour\"] = full_df.date.dt.year + full_df[\"weekday\"] * 24 + full_df[\"hour\"]\n",
    "\n",
    "    full_df = full_df.sort_values(\"date\").reset_index(drop=True)\n",
    "    \n",
    "    full_df[\"hour_avg_speed\"] = full_df.groupby(\"hour\")[\"truth\"].transform(\"mean\")\n",
    "    full_df[\"hour_std_speed\"] = full_df.groupby(\"hour\")[\"truth\"].transform(\"std\")\n",
    "\n",
    "    full_df[\"weekday_hour_avg_speed\"] = full_df.groupby([\"weekday_hour\"])[\"truth\"].transform(\"mean\")\n",
    "    full_df[\"weekday_hour_std_speed\"] = full_df.groupby([\"weekday_hour\"])[\"truth\"].transform(\"std\")\n",
    "\n",
    "    full_df[\"per_change\"] = full_df[\"speed\"].pct_change()\n",
    "    \n",
    "    full_df = fill_speed_func(full_df)\n",
    "\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T04:01:59.947913Z",
     "iopub.status.busy": "2020-12-05T04:01:59.946543Z",
     "iopub.status.idle": "2020-12-05T04:01:59.950735Z",
     "shell.execute_reply": "2020-12-05T04:01:59.951237Z"
    },
    "papermill": {
     "duration": 0.028649,
     "end_time": "2020-12-05T04:01:59.951418",
     "exception": false,
     "start_time": "2020-12-05T04:01:59.922769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def prepare(full_df, fill_speed_func):\n",
    "\n",
    "    \n",
    "    full_df = feature_engineering(full_df, fill_speed_func)\n",
    "    train_df = full_df[~full_df.speed.isna()].reset_index(drop=True)\n",
    "    valid_df_2017 = full_df[(full_df.speed.isna()) & (full_df.date.dt.year==2017)].reset_index(drop=True)\n",
    "    valid_df_2018 = full_df[(~full_df.speed.isna()) & (full_df.date.dt.year==2018)].reset_index(drop=True)\n",
    "    test_df = full_df[(full_df.speed.isna()) & (full_df.date.dt.year==2018)].reset_index(drop=True)\n",
    "    \n",
    "    print(train_df.shape, valid_df_2017.shape, valid_df_2018.shape)\n",
    "    \n",
    "    selected_features = [col for col in train_df.columns if col not in [\n",
    "        \"id\", \"date\", \"per_change\", \"daily_max\", \"daily_min\",\n",
    "        \"day_weekday_hour\",\n",
    "        \"date_only\",\n",
    "        \"year_weekday_hour\",\n",
    "        \"speed_fill_median_h\",\n",
    "        \"speed_fill_median_wh\",\n",
    "        \"speed_fill_mean_h\",\n",
    "        \"speed_fill_mean_wh\",\n",
    "        \"speed_fill_mean\",\n",
    "        \"speed_fill_median\", \n",
    "        \"speed_fill_wh_median\",\n",
    "        \"lin_speed\", \"quadratic_speed\", \"nearest_speed\", \"speed\", \"is_test\", \"truth\"\n",
    "    ]]\n",
    "    \n",
    "    # fill nans\n",
    "    for col in selected_features:\n",
    "        avg = train_df[col].mean()\n",
    "        for df in [train_df, test_df, valid_df_2017, valid_df_2018]:\n",
    "            df[col] = df[col].fillna(avg)\n",
    "    \n",
    "    return train_df, valid_df_2017, valid_df_2018, test_df, selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T04:02:00.001758Z",
     "iopub.status.busy": "2020-12-05T04:01:59.996642Z",
     "iopub.status.idle": "2020-12-05T04:02:00.032863Z",
     "shell.execute_reply": "2020-12-05T04:02:00.032468Z"
    },
    "papermill": {
     "duration": 0.065526,
     "end_time": "2020-12-05T04:02:00.032942",
     "exception": false,
     "start_time": "2020-12-05T04:01:59.967416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(model_builder: Callable,\n",
    "          train_df, \n",
    "          valid_df_2017,\n",
    "          valid_df_2018,\n",
    "          test_df,\n",
    "          features: List[str],\n",
    "          scaling=True,\n",
    "          one_hot_features=[]):\n",
    "    all_train_preds = np.array([])\n",
    "    all_train_y = np.array([])\n",
    "    \n",
    "    all_valid_preds_2017 = np.array([])\n",
    "    all_valid_y_2017 = np.array([])\n",
    "    all_valid_preds_2018 = np.array([])\n",
    "    all_valid_y_2018 = np.array([])\n",
    "    \n",
    "    all_test_preds = 0\n",
    "    \n",
    "    models = []\n",
    "    num_fold = 1\n",
    "    feature_importance = None\n",
    "    \n",
    "    for fold in range(num_fold):\n",
    "        _train_df = train_df.copy()\n",
    "        _valid_df_2017 = valid_df_2017.copy()\n",
    "        _valid_df_2018 = valid_df_2018.copy()\n",
    "        \n",
    "        _features = features.copy()\n",
    "        _features = [f for f in _features if f not in one_hot_features]\n",
    "        \n",
    "        train_ohe = {}\n",
    "        valid_2017_ohe = {}\n",
    "        valid_2018_ohe = {}\n",
    "        test_ohe = {}\n",
    "        for f in one_hot_features:\n",
    "            labeler = LabelEncoder()\n",
    "            labeler.fit(_train_df[f])\n",
    "            \n",
    "            train_ohe[f] = tf.keras.utils.to_categorical(labeler.transform(_train_df[f]))\n",
    "            valid_2017_ohe[f] = tf.keras.utils.to_categorical(labeler.transform(_valid_df_2017[f]))\n",
    "            valid_2018_ohe[f] = tf.keras.utils.to_categorical(labeler.transform(_valid_df_2018[f]))\n",
    "            test_ohe[f] = tf.keras.utils.to_categorical(labeler.transform(test_df[f]))\n",
    "            \n",
    "        _train_x = _train_df[_features].values\n",
    "        _valid_x_2017 = _valid_df_2017[_features].values\n",
    "        _valid_x_2018 = _valid_df_2018[_features].values\n",
    "        _test_x = test_df[_features].copy().values\n",
    "        \n",
    "        if scaling:\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(_train_x)\n",
    "            \n",
    "            _train_x = scaler.transform(_train_x)\n",
    "            _valid_x_2017 = scaler.transform(_valid_x_2017)\n",
    "            _valid_x_2018 = scaler.transform(_valid_x_2018)\n",
    "            _test_x = scaler.transform(_test_x)\n",
    "        \n",
    "        for ohe_f in train_ohe.keys():\n",
    "            _train_x = np.concatenate([_train_x, train_ohe[ohe_f]], axis=1)\n",
    "            _valid_x_2017 = np.concatenate([_valid_x_2017, valid_2017_ohe[ohe_f]], axis=1)\n",
    "            _valid_x_2018 = np.concatenate([_valid_x_2018, valid_2018_ohe[ohe_f]], axis=1)\n",
    "            _test_x = np.concatenate([_test_x, test_ohe[ohe_f]], axis=1)\n",
    "        \n",
    "        _train_y = _train_df.speed.values\n",
    "        _valid_y_2017 = _valid_df_2017.truth.values\n",
    "        _valid_y_2018 = _valid_df_2018.truth.values            \n",
    "        \n",
    "        model = model_builder()\n",
    "        num_seed = 5\n",
    "        if isinstance(model, tf.keras.Model):\n",
    "            for seed in range(0, num_seed):\n",
    "                model = model_builder()\n",
    "                model.compile(\"adam\", loss=\"mse\")\n",
    "                history = model.fit(\n",
    "                    _train_x,\n",
    "                    _train_y,\n",
    "                    validation_data=(\n",
    "                        _valid_x_2017, _valid_y_2017\n",
    "                    ),\n",
    "                    batch_size=200, epochs=100,\n",
    "                    verbose=0,\n",
    "                    callbacks=[\n",
    "                        CosineAnnealingLearningRateSchedule(100, 10, 0.001),\n",
    "                        tf.keras.callbacks.ModelCheckpoint(f'model_{fold}_{seed}.h5', save_best_only=True, verbose=0, monitor='val_loss'),\n",
    "                    ]\n",
    "                )\n",
    "                print(min(history.history[\"val_loss\"]))\n",
    "        elif isinstance(model, lgb.LGBMRegressor):\n",
    "            sample_weights = [1 if 20 <= h <= 23 else 1 for h in train_df.hour]\n",
    "            model.fit(\n",
    "                _train_x,\n",
    "                _train_y,\n",
    "                eval_set=(\n",
    "                    _valid_x_2017, _valid_y_2017\n",
    "                ),\n",
    "                early_stopping_rounds=100,\n",
    "                verbose=False,\n",
    "                sample_weight=sample_weights,\n",
    "            )\n",
    "            \n",
    "            feature_importance = model.feature_importances_\n",
    "        elif isinstance(model, ctb.CatBoostRegressor):\n",
    "            model.fit(\n",
    "                _train_x,\n",
    "                _train_y,\n",
    "                eval_set=(\n",
    "                    _valid_x_2017, _valid_y_2017\n",
    "                ),\n",
    "                early_stopping_rounds=100,\n",
    "                verbose=False,\n",
    "            ) \n",
    "        else:\n",
    "            model.fit(\n",
    "                _train_x,\n",
    "                _train_y,\n",
    "            )\n",
    "        models.append(model)\n",
    "        \n",
    "        train_preds = 0\n",
    "        valid_preds_2017 = 0\n",
    "        valid_preds_2018 = 0\n",
    "        test_preds = 0       \n",
    "        if isinstance(model, tf.keras.Model):\n",
    "            for seed in range(num_seed):\n",
    "                model.load_weights(f'model_{fold}_{seed}.h5')\n",
    "                train_preds += model.predict(_train_x).reshape(-1) / num_seed\n",
    "                valid_preds_2017 += model.predict(_valid_x_2017).reshape(-1) / num_seed\n",
    "                valid_preds_2018 += model.predict(_valid_x_2018).reshape(-1) / num_seed\n",
    "                test_preds += model.predict(_test_x).reshape(-1) / num_seed\n",
    "        \n",
    "        else:\n",
    "            train_preds = model.predict(_train_x).reshape(-1)\n",
    "            valid_preds_2017 = model.predict(_valid_x_2017).reshape(-1)\n",
    "            valid_preds_2018 = model.predict(_valid_x_2018).reshape(-1)\n",
    "            test_preds = model.predict(_test_x).reshape(-1)\n",
    "        all_test_preds += test_preds / num_fold\n",
    "\n",
    "\n",
    "        train_mse = mean_squared_error(_train_y, train_preds)\n",
    "        valid_mse_2017 = mean_squared_error(_valid_y_2017, valid_preds_2017)\n",
    "        valid_mse_2018 = mean_squared_error(_valid_y_2018, valid_preds_2018)\n",
    "#         print(train_mse, valid_mse_2017, valid_mse_2018)\n",
    "\n",
    "#         print(train_preds.shape)\n",
    "        all_train_preds = np.concatenate([all_train_preds, train_preds]) \n",
    "        all_train_y = np.concatenate([all_train_y, _train_y])\n",
    "\n",
    "        all_valid_preds_2017 = np.concatenate([all_valid_preds_2017, valid_preds_2017]) \n",
    "        all_valid_y_2017 = np.concatenate([all_valid_y_2017, _valid_y_2017]) \n",
    "\n",
    "        all_valid_preds_2018 = np.concatenate([all_valid_preds_2018, valid_preds_2018]) \n",
    "        all_valid_y_2018 = np.concatenate([all_valid_y_2018, _valid_y_2018]) \n",
    "    \n",
    "    train_mse = mean_squared_error(all_train_y, all_train_preds)\n",
    "    valid_mse_2017 = mean_squared_error(all_valid_y_2017, all_valid_preds_2017)\n",
    "    valid_mse_2018 = mean_squared_error(all_valid_y_2018, all_valid_preds_2018)\n",
    "    \n",
    "    print(train_mse, valid_mse_2017, valid_mse_2018)\n",
    "    return {\n",
    "        \"models\": models,\n",
    "        \"all_train_preds\": all_train_preds,\n",
    "        \"all_train_y\": all_train_y,\n",
    "        \n",
    "        \"all_valid_preds_2017\": all_valid_preds_2017,\n",
    "        \"all_valid_y_2017\": all_valid_y_2017,\n",
    "        \"all_valid_preds_2018\": all_valid_preds_2018,\n",
    "        \"all_valid_y_2018\": all_valid_y_2018,\n",
    "        \"test_preds\": all_test_preds,\n",
    "        \"features\": _features,\n",
    "        \"feature_importance\": feature_importance, \n",
    "        \"valid_mse_2017\": valid_mse_2017,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T04:02:00.074405Z",
     "iopub.status.busy": "2020-12-05T04:02:00.069067Z",
     "iopub.status.idle": "2020-12-05T04:02:00.088524Z",
     "shell.execute_reply": "2020-12-05T04:02:00.089021Z"
    },
    "papermill": {
     "duration": 0.045043,
     "end_time": "2020-12-05T04:02:00.089174",
     "exception": false,
     "start_time": "2020-12-05T04:02:00.044131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_models(train_df, valid_df_2017, valid_df_2018, test_df, selected_features):\n",
    "    init = tf.keras.initializers.LecunNormal()\n",
    "    keras_one_hot_features = [\"hour\", \"weekday\"]\n",
    "    \n",
    "    model_builders = {\n",
    "        \"lgb\": lambda :lgb.LGBMRegressor(\n",
    "                boosting_type='gbdt', \n",
    "                objective=\"mse\",\n",
    "                min_child_samples=20,\n",
    "                num_leaves=40,\n",
    "                max_depth=16,\n",
    "                learning_rate=0.05,\n",
    "                n_estimators=1000,\n",
    "                colsample_bytree=.4,\n",
    "                subsample=1.0,\n",
    "                subsample_freq=0,\n",
    "                importance_type=\"gain\",\n",
    "                random_state=random_state,\n",
    "                n_jobs=-1,\n",
    "        ),\n",
    "        'lgbrf': lambda :lgb.LGBMRegressor(\n",
    "                boosting_type='rf', \n",
    "                objective=\"mse\",\n",
    "                min_child_samples=20,\n",
    "                num_leaves=128,\n",
    "                max_depth=16,\n",
    "                learning_rate=0.05,\n",
    "                n_estimators=1000,\n",
    "                colsample_bytree=.5,\n",
    "                subsample=.95,\n",
    "                subsample_freq=20,\n",
    "                importance_type=\"gain\",\n",
    "                random_state=random_state,\n",
    "                n_jobs=-1,\n",
    "        ),\n",
    "        'cat': lambda :ctb.CatBoostRegressor(\n",
    "                loss_function=\"RMSE\",\n",
    "                learning_rate=.06,\n",
    "                max_depth=8,\n",
    "                min_child_samples=20,\n",
    "                colsample_bylevel=1.0,\n",
    "                n_estimators=None,\n",
    "                use_best_model=True,\n",
    "                random_seed=random_state,\n",
    "        ),\n",
    "        'keras': lambda :tf.keras.Sequential(\n",
    "                    layers=[\n",
    "                        tf.keras.Input(shape=(len(selected_features) - len(keras_one_hot_features) + sum([full_df[f].nunique() for f in keras_one_hot_features]),)),  \n",
    "\n",
    "                        tf.keras.layers.Dense(20, kernel_initializer=init, activation=tfa.activations.mish),\n",
    "                        tf.keras.layers.Dropout(.05),\n",
    "                        tf.keras.layers.BatchNormalization(),\n",
    "                        tf.keras.layers.GaussianNoise(.15),\n",
    "\n",
    "                        tf.keras.layers.Dense(20, kernel_initializer=init, activation=\"tanh\"),\n",
    "                        tf.keras.layers.Dropout(.05),\n",
    "                        tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "                        tf.keras.layers.Dense(1, kernel_initializer=init, activation=\"linear\"),\n",
    "                    ]\n",
    "        ),\n",
    "        'lr': lambda :LinearRegression(),\n",
    "        'rr': lambda :Ridge(),\n",
    "        'sdg': lambda :SGDRegressor(max_iter=800),\n",
    "        'etr': lambda :ExtraTreesRegressor(\n",
    "                    n_estimators=400,\n",
    "                    max_depth=18,\n",
    "                    max_leaf_nodes=None,\n",
    "                    criterion=\"mse\",\n",
    "                    min_samples_split=25,\n",
    "                    max_features=.9,\n",
    "                    bootstrap=False,\n",
    "                    random_state=random_state,\n",
    "                    n_jobs=-1,\n",
    "        ),\n",
    "        'hgbr': lambda :HistGradientBoostingRegressor(\n",
    "                    max_iter=200,\n",
    "                    max_leaf_nodes=64,\n",
    "                    max_depth=16,\n",
    "                    learning_rate=.04,\n",
    "                    random_state=random_state,\n",
    "        ),\n",
    "        'svr': lambda :SVR(C=300),\n",
    "        'mlp': lambda :\n",
    "                MLPRegressor(\n",
    "                    hidden_layer_sizes=(16, 16), activation='tanh', solver='adam', \n",
    "                    learning_rate=\"adaptive\", shuffle=True,\n",
    "                    max_iter=120, random_state=random_state,\n",
    "        ),\n",
    "        'gbr': lambda :GradientBoostingRegressor(\n",
    "            subsample=.8,\n",
    "            n_estimators=120,\n",
    "            min_samples_leaf=30, \n",
    "            max_depth=8,\n",
    "            learning_rate=.04,\n",
    "            criterion=\"mse\",\n",
    "            min_samples_split=20,\n",
    "            random_state=random_state,\n",
    "        ),\n",
    "        'rf': lambda :RandomForestRegressor(\n",
    "            n_estimators=400,\n",
    "            max_depth=12,\n",
    "            min_samples_leaf=6,\n",
    "            max_features=.4,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "        ),\n",
    "        'knn': lambda :KNeighborsRegressor(\n",
    "            n_neighbors=20,\n",
    "            weights=\"distance\",\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    print(\"#\"*5, \"Models\", \"#\"*5)\n",
    "    model_results = {}\n",
    "    for key, model_builder in model_builders.items():\n",
    "        scaling = False\n",
    "        one_hot_features = []\n",
    "        \n",
    "        if key == 'keras':\n",
    "            one_hot_features = keras_one_hot_features\n",
    "        \n",
    "        if key in ['keras', 'lr', 'rr', 'sdg', 'mlp', 'rf', 'knn']:\n",
    "            scaling = True\n",
    "        \n",
    "        print(key)\n",
    "        result = train(\n",
    "            model_builder,\n",
    "            train_df,\n",
    "            valid_df_2017,\n",
    "            valid_df_2018,\n",
    "            test_df,\n",
    "            selected_features,\n",
    "            scaling=scaling,\n",
    "            one_hot_features=one_hot_features,\n",
    "        )\n",
    "        model_results[key] = result\n",
    "    return model_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T04:02:00.137463Z",
     "iopub.status.busy": "2020-12-05T04:02:00.135499Z",
     "iopub.status.idle": "2020-12-05T04:02:00.140766Z",
     "shell.execute_reply": "2020-12-05T04:02:00.140197Z"
    },
    "papermill": {
     "duration": 0.035338,
     "end_time": "2020-12-05T04:02:00.140870",
     "exception": false,
     "start_time": "2020-12-05T04:02:00.105532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def blending(model_results):\n",
    "    train_preds_df = pd.DataFrame({\n",
    "        k: r[\"all_train_preds\"] for k, r in model_results.items()\n",
    "    })\n",
    "    train_preds_df[\"y\"] = model_results[\"lr\"][\"all_train_y\"]\n",
    "    \n",
    "    \n",
    "    oof_df_2017 = pd.DataFrame({\n",
    "        k: r[\"all_valid_preds_2017\"] for k, r in model_results.items()\n",
    "    })\n",
    "    oof_df_2017[\"y\"] = model_results[\"lr\"][\"all_valid_y_2017\"]\n",
    "\n",
    "    \n",
    "    oof_cols = [col for col in oof_df_2017.columns if col not in [\"y\"]]\n",
    "    \n",
    "    blending_lr = LinearRegression()\n",
    "\n",
    "    # train blender with 2017 data\n",
    "    blending_lr.fit(\n",
    "        oof_df_2017[oof_cols].values,\n",
    "        oof_df_2017[\"y\"].values,\n",
    "    )\n",
    "    \n",
    "\n",
    "#     display(pd.DataFrame({\n",
    "#         \"model\": oof_cols,\n",
    "#         \"weight\": blending_lr.coef_\n",
    "#     }).sort_values(\"weight\"))\n",
    "\n",
    "    # blend valid\n",
    "    oof_df_2017[\"blended\"] =  0\n",
    "    for i in range(len(oof_cols)):\n",
    "        oof_df_2017[\"blended\"] += oof_df_2017[oof_cols[i]] * blending_lr.coef_[i]\n",
    "    oof_df_2017[\"blended\"] += blending_lr.intercept_\n",
    "    \n",
    "    # blend train\n",
    "    train_preds_df[\"blended\"] =  0\n",
    "    for i in range(len(oof_cols)):\n",
    "        train_preds_df[\"blended\"] += train_preds_df[oof_cols[i]] * blending_lr.coef_[i]\n",
    "    train_preds_df[\"blended\"] += blending_lr.intercept_\n",
    "\n",
    "    \n",
    "    print(\"Blending CV\")\n",
    "    mse_2017 = mean_squared_error(oof_df_2017[\"y\"], oof_df_2017[\"blended\"])\n",
    "    print(mean_squared_error(train_preds_df[\"y\"], train_preds_df[\"blended\"]), mse_2017)\n",
    "    \n",
    "    # blend submissions\n",
    "    sub_df = pd.DataFrame({\n",
    "        k: r[\"test_preds\"] for k, r in model_results.items()\n",
    "    })\n",
    "    \n",
    "    sub_df[\"blended\"] =  0\n",
    "    for i in range(len(oof_cols)):\n",
    "        sub_df[\"blended\"] += sub_df[oof_cols[i]] * blending_lr.coef_[i]\n",
    "    sub_df[\"blended\"] += blending_lr.intercept_\n",
    "\n",
    "    test_df[\"speed\"] = sub_df[\"blended\"].values\n",
    "#     test_df[[\"id\", \"speed\"]].to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "    return train_preds_df, oof_df_2017, test_df[[\"id\", \"speed\"]], mse_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T04:02:00.177962Z",
     "iopub.status.busy": "2020-12-05T04:02:00.177201Z",
     "iopub.status.idle": "2020-12-05T04:02:00.180621Z",
     "shell.execute_reply": "2020-12-05T04:02:00.181096Z"
    },
    "papermill": {
     "duration": 0.024348,
     "end_time": "2020-12-05T04:02:00.181224",
     "exception": false,
     "start_time": "2020-12-05T04:02:00.156876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015503,
     "end_time": "2020-12-05T04:02:00.212872",
     "exception": false,
     "start_time": "2020-12-05T04:02:00.197369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T04:02:00.256524Z",
     "iopub.status.busy": "2020-12-05T04:02:00.253292Z",
     "iopub.status.idle": "2020-12-05T04:07:50.376094Z",
     "shell.execute_reply": "2020-12-05T04:07:50.375420Z"
    },
    "papermill": {
     "duration": 350.14741,
     "end_time": "2020-12-05T04:07:50.376207",
     "exception": false,
     "start_time": "2020-12-05T04:02:00.228797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Seed 2020 -----\n",
      "Missing intervals 2018\n",
      "0 days 01:00:00    3135\n",
      "0 days 02:00:00    1260\n",
      "0 days 03:00:00     526\n",
      "0 days 04:00:00     217\n",
      "0 days 05:00:00      75\n",
      "0 days 06:00:00      30\n",
      "0 days 07:00:00       8\n",
      "0 days 10:00:00       2\n",
      "0 days 08:00:00       2\n",
      "0 days 09:00:00       1\n",
      "Name: interval, dtype: int64\n",
      "2      2018-01-01 02:00:00+08:00\n",
      "5      2018-01-01 05:00:00+08:00\n",
      "7      2018-01-01 07:00:00+08:00\n",
      "8      2018-01-01 08:00:00+08:00\n",
      "10     2018-01-01 10:00:00+08:00\n",
      "                  ...           \n",
      "8753   2018-12-31 17:00:00+08:00\n",
      "8755   2018-12-31 19:00:00+08:00\n",
      "8757   2018-12-31 21:00:00+08:00\n",
      "8758   2018-12-31 22:00:00+08:00\n",
      "8759   2018-12-31 23:00:00+08:00\n",
      "Name: date, Length: 3504, dtype: datetime64[ns, Hongkong]\n",
      "2      2017-01-01 02:00:00+08:00\n",
      "5      2017-01-01 05:00:00+08:00\n",
      "7      2017-01-01 07:00:00+08:00\n",
      "8      2017-01-01 08:00:00+08:00\n",
      "10     2017-01-01 10:00:00+08:00\n",
      "                  ...           \n",
      "8753   2017-12-31 17:00:00+08:00\n",
      "8755   2017-12-31 19:00:00+08:00\n",
      "8757   2017-12-31 21:00:00+08:00\n",
      "8758   2017-12-31 22:00:00+08:00\n",
      "8759   2017-12-31 23:00:00+08:00\n",
      "Name: date, Length: 3504, dtype: datetime64[ns, Hongkong]\n",
      "(10509, 98) (3497, 98) (5256, 98)\n",
      "##### Models #####\n",
      "lgb\n",
      "6.844240870610412 9.526547771307529 6.9480764585806\n",
      "lgbrf\n",
      "8.333639555216294 9.836052298068438 8.506445171132361\n",
      "cat\n",
      "6.061930406433417 9.559015780036361 6.076409636758995\n",
      "keras\n",
      "9.575371742248535\n",
      "9.51912784576416\n",
      "9.757516860961914\n",
      "9.674549102783203\n",
      "9.622957229614258\n",
      "8.798178252426492 9.341337487016688 8.95335533974945\n",
      "lr\n",
      "11.131366449253624 10.305286941979993 11.27408986905921\n",
      "rr\n",
      "11.124680181569406 10.302641715419563 11.261224070254995\n",
      "sdg\n",
      "11.421370847545619 10.604314911186513 11.471767455683965\n",
      "etr\n",
      "4.426988328357204 9.64777676176284 4.4387570850036875\n",
      "hgbr\n",
      "5.424950208767479 9.504139592476998 5.537005622086705\n",
      "svr\n",
      "10.332443809985316 10.325997313130818 10.339637908339208\n",
      "mlp\n",
      "9.159941172200016 10.575261082883488 9.159650994750635\n",
      "gbr\n",
      "6.598179101945332 9.37733453969807 6.732136270926095\n",
      "rf\n",
      "6.084083934731415 9.511657583316579 6.213211220246654\n",
      "knn\n",
      "0.0 11.773999275975948 0.0\n",
      "Blending CV\n",
      "6.994287887295554 9.118098199685821\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fill_speed_func(df):\n",
    "    df[\"speed_fill_median\"] = df.groupby(\"weekday_hour\")[\"speed\"].transform(\"median\")\n",
    "    df.loc[~full_df.speed.isna(), \"speed_fill_median\"] = df.loc[~df.speed.isna(), \"speed\"]\n",
    "    df = speed_feature_engineering(df, 'speed_fill_median')\n",
    "    return df\n",
    "\n",
    "subs = []\n",
    "mse_2017_mean = 0\n",
    "num_fold = 1\n",
    "for i in range(2020, 2020 + num_fold):\n",
    "    print(\"-\"*5, \"Seed\", i, \"-\"*5)\n",
    "    sample_submission_df, train_df, test_df, all_df = read_files()\n",
    "    train_df, test_df, full_df, full_2017_df, full_2018_df = handle_missing_dates(\n",
    "        train_df, test_df, all_df, hole_random_state=i)\n",
    "    train_df, valid_df_2017, valid_df_2018, test_df, selected_features = prepare(full_df, fill_speed_func)\n",
    "    model_results = train_models(train_df, valid_df_2017, valid_df_2018, test_df, selected_features)\n",
    "    \n",
    "    blended_train_preds, blended_val_preds, blended_sub, mse_2017 = blending(model_results)\n",
    "    blended_sub[[\"id\", \"speed\"]].to_csv(f\"submission_{i}.csv\", index=False)\n",
    "    subs.append(blended_sub[[\"id\", \"speed\"]])\n",
    "    \n",
    "    mse_2017_mean += mse_2017 / num_fold\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T04:07:50.433319Z",
     "iopub.status.busy": "2020-12-05T04:07:50.432717Z",
     "iopub.status.idle": "2020-12-05T04:07:50.438461Z",
     "shell.execute_reply": "2020-12-05T04:07:50.438952Z"
    },
    "papermill": {
     "duration": 0.037418,
     "end_time": "2020-12-05T04:07:50.439096",
     "exception": false,
     "start_time": "2020-12-05T04:07:50.401678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.118098199685821"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_2017_mean\n",
    "\n",
    "# 9.127798243957113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T04:07:50.496106Z",
     "iopub.status.busy": "2020-12-05T04:07:50.495469Z",
     "iopub.status.idle": "2020-12-05T04:07:50.498919Z",
     "shell.execute_reply": "2020-12-05T04:07:50.499415Z"
    },
    "papermill": {
     "duration": 0.035498,
     "end_time": "2020-12-05T04:07:50.499537",
     "exception": false,
     "start_time": "2020-12-05T04:07:50.464039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_preds = 0\n",
    "sub_df = pd.DataFrame({\n",
    "    \"id\": subs[0][\"id\"],\n",
    "    \"speed\": 0,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T04:07:50.556648Z",
     "iopub.status.busy": "2020-12-05T04:07:50.555689Z",
     "iopub.status.idle": "2020-12-05T04:07:50.559167Z",
     "shell.execute_reply": "2020-12-05T04:07:50.558536Z"
    },
    "papermill": {
     "duration": 0.034992,
     "end_time": "2020-12-05T04:07:50.559294",
     "exception": false,
     "start_time": "2020-12-05T04:07:50.524302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(subs)):\n",
    "    sub_df[\"speed\"] += subs[i][\"speed\"].values / len(subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T04:07:50.619199Z",
     "iopub.status.busy": "2020-12-05T04:07:50.618584Z",
     "iopub.status.idle": "2020-12-05T04:07:50.628870Z",
     "shell.execute_reply": "2020-12-05T04:07:50.627995Z"
    },
    "papermill": {
     "duration": 0.043705,
     "end_time": "2020-12-05T04:07:50.629010",
     "exception": false,
     "start_time": "2020-12-05T04:07:50.585305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>48.105308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>47.420516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>36.790235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>29.867004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>40.398620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      speed\n",
       "0   0  48.105308\n",
       "1   1  47.420516\n",
       "2   2  36.790235\n",
       "3   3  29.867004\n",
       "4   4  40.398620"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T04:07:50.714049Z",
     "iopub.status.busy": "2020-12-05T04:07:50.712984Z",
     "iopub.status.idle": "2020-12-05T04:07:50.729726Z",
     "shell.execute_reply": "2020-12-05T04:07:50.728831Z"
    },
    "papermill": {
     "duration": 0.06169,
     "end_time": "2020-12-05T04:07:50.729860",
     "exception": false,
     "start_time": "2020-12-05T04:07:50.668170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 363.320374,
   "end_time": "2020-12-05T04:07:50.877361",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-05T04:01:47.556987",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
