{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-12-05T05:32:29.658907Z",
     "iopub.status.busy": "2020-12-05T05:32:29.657947Z",
     "iopub.status.idle": "2020-12-05T05:32:30.995834Z",
     "shell.execute_reply": "2020-12-05T05:32:30.994232Z"
    },
    "papermill": {
     "duration": 1.36815,
     "end_time": "2020-12-05T05:32:30.996045",
     "exception": false,
     "start_time": "2020-12-05T05:32:29.627895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/msbd5001-fall2020/sampleSubmission.csv\n",
      "/kaggle/input/msbd5001-fall2020/train.csv\n",
      "/kaggle/input/msbd5001-fall2020/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import lightgbm as lgb \n",
    "import catboost as ctb\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T05:32:31.058064Z",
     "iopub.status.busy": "2020-12-05T05:32:31.054291Z",
     "iopub.status.idle": "2020-12-05T05:32:37.702226Z",
     "shell.execute_reply": "2020-12-05T05:32:37.701324Z"
    },
    "papermill": {
     "duration": 6.68447,
     "end_time": "2020-12-05T05:32:37.702357",
     "exception": false,
     "start_time": "2020-12-05T05:32:31.017887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.3.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.3.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import gc\n",
    "from math import pi\n",
    "from math import cos\n",
    "from math import floor\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from typing import Callable, List\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, BayesianRidge, Lasso, HuberRegressor, ElasticNet, Lars\n",
    "from sklearn.svm import LinearSVR, SVR, NuSVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor, BaggingRegressor\n",
    "\n",
    "\n",
    "class CosineAnnealingLearningRateSchedule(tf.keras.callbacks.Callback):\n",
    "\t# constructor\n",
    "\tdef __init__(self, n_epochs, n_cycles, lrate_max, verbose=0):\n",
    "\t\tself.epochs = n_epochs\n",
    "\t\tself.cycles = n_cycles\n",
    "\t\tself.lr_max = lrate_max\n",
    "\t\tself.lrates = list()\n",
    " \n",
    "\t# calculate learning rate for an epoch\n",
    "\tdef cosine_annealing(self, epoch, n_epochs, n_cycles, lrate_max):\n",
    "\t\tepochs_per_cycle = floor(n_epochs/n_cycles)\n",
    "\t\tcos_inner = (pi * (epoch % epochs_per_cycle)) / (epochs_per_cycle)\n",
    "\t\treturn lrate_max/2 * (cos(cos_inner) + 1)\n",
    " \n",
    "\t# calculate and set learning rate at the start of the epoch\n",
    "\tdef on_epoch_begin(self, epoch, logs=None):\n",
    "\t\t# calculate learning rate\n",
    "\t\tlr = self.cosine_annealing(epoch, self.epochs, self.cycles, self.lr_max)\n",
    "\t\t# set learning rate\n",
    "\t\ttf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "\t\t# log value\n",
    "\t\tself.lrates.append(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T05:32:37.757680Z",
     "iopub.status.busy": "2020-12-05T05:32:37.752928Z",
     "iopub.status.idle": "2020-12-05T05:32:37.815197Z",
     "shell.execute_reply": "2020-12-05T05:32:37.814414Z"
    },
    "papermill": {
     "duration": 0.091919,
     "end_time": "2020-12-05T05:32:37.815323",
     "exception": false,
     "start_time": "2020-12-05T05:32:37.723404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "holidays =[\n",
    "    \"2017-1-1\",\n",
    "    \"2017-1-2\",\n",
    "    \"2017-1-28\",\n",
    "    \"2017-1-30\",\n",
    "    \"2017-1-31\",\n",
    "    \"2017-4-4\",\n",
    "    \"2017-4-14\",\n",
    "    \"2017-4-15\",\n",
    "    \"2017-4-17\",\n",
    "    \"2017-5-1\",\n",
    "    \"2017-5-3\",\n",
    "    \"2017-5-30\",\n",
    "    \"2017-7-1\",\n",
    "    \"2017-10-1\",\n",
    "    \"2017-10-2\",\n",
    "    \"2017-10-5\",\n",
    "    \"2017-10-28\",\n",
    "    \"2017-12-25\",\n",
    "    \"2017-12-26\",\n",
    "    \"2018-1-1\",\n",
    "    \"2018-2-16\",\n",
    "    \"2018-2-17\",\n",
    "    \"2018-2-19\",\n",
    "    \"2018-3-30\",\n",
    "    \"2018-3-31\",\n",
    "    \"2018-4-2\",\n",
    "    \"2018-4-5\",\n",
    "    \"2018-5-1\",\n",
    "    \"2018-5-22\",\n",
    "    \"2018-6-18\",\n",
    "    \"2018-7-1\",\n",
    "    \"2018-7-2\",\n",
    "    \"2018-9-24\",\n",
    "    \"2018-9-25\",\n",
    "    \"2018-10-1\",\n",
    "    \"2018-10-17\",\n",
    "    \"2018-12-25\",\n",
    "    \"2018-12-26\",\n",
    "]\n",
    "holidays = [pd.to_datetime(h, format='%Y-%m-%d').tz_localize(\"HongKong\") for h in holidays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T05:32:37.863789Z",
     "iopub.status.busy": "2020-12-05T05:32:37.862685Z",
     "iopub.status.idle": "2020-12-05T05:32:37.866122Z",
     "shell.execute_reply": "2020-12-05T05:32:37.865359Z"
    },
    "papermill": {
     "duration": 0.029814,
     "end_time": "2020-12-05T05:32:37.866249",
     "exception": false,
     "start_time": "2020-12-05T05:32:37.836435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "random_state = 2020\n",
    "# num_fold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T05:32:37.920407Z",
     "iopub.status.busy": "2020-12-05T05:32:37.919276Z",
     "iopub.status.idle": "2020-12-05T05:32:37.922268Z",
     "shell.execute_reply": "2020-12-05T05:32:37.922918Z"
    },
    "papermill": {
     "duration": 0.035359,
     "end_time": "2020-12-05T05:32:37.923078",
     "exception": false,
     "start_time": "2020-12-05T05:32:37.887719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_files():\n",
    "    sample_submission_df = pd.read_csv(\"/kaggle/input/msbd5001-fall2020/sampleSubmission.csv\")\n",
    "    sample_submission_df.shape\n",
    "\n",
    "    train_df = pd.read_csv(\"/kaggle/input/msbd5001-fall2020/train.csv\")\n",
    "    test_df = pd.read_csv(\"/kaggle/input/msbd5001-fall2020/test.csv\")\n",
    "    train_df.date = pd.to_datetime(train_df.date, format='%d/%m/%Y %H:%M').dt.tz_localize(\"HongKong\")\n",
    "    test_df.date = pd.to_datetime(test_df.date, format='%d/%m/%Y %H:%M').dt.tz_localize(\"HongKong\")\n",
    "    \n",
    "    test_df[\"is_test\"] = True\n",
    "    train_df[\"is_test\"] = False\n",
    "\n",
    "    all_df = pd.concat([train_df, test_df], ignore_index=True).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    return sample_submission_df, train_df, test_df, all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-12-05T05:32:37.990839Z",
     "iopub.status.busy": "2020-12-05T05:32:37.980458Z",
     "iopub.status.idle": "2020-12-05T05:32:37.994477Z",
     "shell.execute_reply": "2020-12-05T05:32:37.993775Z"
    },
    "papermill": {
     "duration": 0.050033,
     "end_time": "2020-12-05T05:32:37.994604",
     "exception": false,
     "start_time": "2020-12-05T05:32:37.944571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_missing_dates(train_df, test_df, all_df, hole_random_state):\n",
    "    all_df[\"lag_date\"] = all_df[\"date\"].shift()\n",
    "    all_df[\"interval\"] = (all_df[\"date\"] - all_df[\"lag_date\"])\n",
    "    train_df[\"lag_date\"] = train_df[\"date\"].shift()\n",
    "    train_df[\"interval\"] = (train_df[\"date\"] - train_df[\"lag_date\"])\n",
    "    \n",
    "    print(\"Missing intervals 2018\")\n",
    "    print(train_df[train_df.date.dt.year==2018].interval.value_counts())\n",
    "    \n",
    "    # all dates\n",
    "    ideal_date = pd.period_range(\n",
    "        start=\"2017-01-01 00:00:00\",\n",
    "        end=\"2018-12-31 23:00:00\",\n",
    "        freq=\"H\")\n",
    "    ideal_df = pd.DataFrame({\n",
    "        \"date\": ideal_date.to_timestamp(),\n",
    "    })\n",
    "    ideal_df[\"date\"] = ideal_df[\"date\"].dt.tz_localize(\"HongKong\")\n",
    "\n",
    "    full_df = pd.merge(ideal_df, all_df, on=\"date\", how=\"left\")\n",
    "    \n",
    "    full_df[\"lag_date\"] = full_df[\"date\"].shift()\n",
    "    full_df[\"interval\"] = (full_df[\"date\"] - full_df[\"lag_date\"])\n",
    "    full_df[\"interval\"].value_counts()\n",
    "    full_df = full_df[[\"date\", \"id\", \"speed\", \"is_test\"]]\n",
    "    \n",
    "    \n",
    "    full_df = full_df[~full_df.is_test.isna()]\n",
    "    full_df[\"id\"] = full_df[\"id\"].astype(int)\n",
    "    \n",
    "    full_2017_df = full_df[full_df.date.dt.year==2017].reset_index(drop=True)\n",
    "    full_2018_df = full_df[full_df.date.dt.year==2018].reset_index(drop=True)\n",
    "    full_2017_df.shape, full_2018_df.shape\n",
    "    \n",
    "    date_holes_2018 = full_2018_df[full_2018_df.speed.isna()].date\n",
    "    \n",
    "    # simulate missing intervals in 2017 data\n",
    "#     date_holes = full_2017_df.sample(frac=3504/full_2018_df.shape[0], random_state=hole_random_state).date\n",
    "    date_holes = full_2018_df[full_2018_df.speed.isna()].date.apply(lambda x: x.replace(year=2017))\n",
    "    \n",
    "    print(date_holes_2018)\n",
    "    print(date_holes)\n",
    "    \n",
    "    full_2017_df[\"truth\"] = full_2017_df.speed\n",
    "    full_2017_df.loc[full_2017_df.date.isin(date_holes), \"speed\"] = np.nan\n",
    "    full_2017_df[\"speed\"].isna().sum()\n",
    "    \n",
    "#     temp_df = full_2017_df[~full_2017_df.speed.isna()].reset_index(drop=True)\n",
    "#     temp_df[\"lag_date\"] = temp_df[\"date\"].shift()\n",
    "#     temp_df[\"interval\"] = (temp_df[\"date\"] - temp_df[\"lag_date\"])\n",
    "    \n",
    "#     print(\"Mocking Missing intervals 2017\")\n",
    "#     print(temp_df[temp_df.date.dt.year == 2017][\"interval\"].value_counts())\n",
    "    \n",
    "    full_2018_df[\"truth\"] = full_2018_df.speed\n",
    "    full_df = pd.concat([full_2017_df, full_2018_df], ignore_index=True)\n",
    "    return train_df, test_df, full_df, full_2017_df, full_2018_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T05:32:38.071434Z",
     "iopub.status.busy": "2020-12-05T05:32:38.066289Z",
     "iopub.status.idle": "2020-12-05T05:32:38.085252Z",
     "shell.execute_reply": "2020-12-05T05:32:38.084604Z"
    },
    "papermill": {
     "duration": 0.068644,
     "end_time": "2020-12-05T05:32:38.085428",
     "exception": false,
     "start_time": "2020-12-05T05:32:38.016784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def speed_feature_engineering(df: pd.DataFrame, speed_feature=\"speed\"):\n",
    "    \n",
    "    \n",
    "    # % change\n",
    "    for lag in [1, 2, 3]:\n",
    "        df[f\"Prev_{lag}_%_change\"] = df[\"per_change\"].shift() \n",
    "    \n",
    "    # Prev/Post hours speed compareing to their weekday_hour average\n",
    "    for lag in [1, 2, 3, 24]:\n",
    "        df[f\"Prev_{lag}_{speed_feature}\"] = df[speed_feature].shift(lag)\n",
    "        df[f\"Post_{lag}_{speed_feature}\"] = df[speed_feature].shift(-lag)\n",
    "           \n",
    "        df[f\"Prev_{lag}_weekday_hour_mean\"] = df[\"weekday_hour_avg_speed\"].shift(lag)\n",
    "        df[f\"Post_{lag}_weekday_hour_mean\"] = df[\"weekday_hour_avg_speed\"].shift(-lag)\n",
    "        \n",
    "        df[f\"Prev_{lag}_weekday_hour_std\"] = df[\"weekday_hour_std_speed\"].shift(lag)\n",
    "        df[f\"Post_{lag}_weekday_hour_std\"] = df[\"weekday_hour_std_speed\"].shift(-lag)\n",
    "        \n",
    "        \n",
    "        # Prev/Post speed compare to Prev/Post mean spped\n",
    "        df[f\"Ratio_Prev_{lag}_weekday_hour_mean\"] = df[f\"Prev_{lag}_{speed_feature}\"] / df[f\"Prev_{lag}_weekday_hour_mean\"]\n",
    "        df[f\"Ratio_Post_{lag}_weekday_hour_mean\"] = df[f\"Post_{lag}_{speed_feature}\"] / df[f\"Post_{lag}_weekday_hour_mean\"]\n",
    "        \n",
    "        df[f\"PDiff_Prev_{lag}_weekday_hour_mean\"] = (df[f\"Prev_{lag}_{speed_feature}\"] - df[f\"Prev_{lag}_weekday_hour_mean\"]) / df[f\"Prev_{lag}_weekday_hour_mean\"]\n",
    "        df[f\"PDiff_Post_{lag}_weekday_hour_mean\"] = (df[f\"Post_{lag}_{speed_feature}\"] - df[f\"Post_{lag}_weekday_hour_mean\"]) / df[f\"Post_{lag}_weekday_hour_mean\"]\n",
    "        \n",
    "        df[f\"Z_Score_Prev_{lag}_weekday_hour\"] = (df[f\"Prev_{lag}_{speed_feature}\"] - df[f\"Prev_{lag}_weekday_hour_mean\"]) / df[f\"Prev_{lag}_weekday_hour_std\"]\n",
    "        df[f\"Z_Score_Post_{lag}_weekday_hour\"] = (df[f\"Post_{lag}_{speed_feature}\"] - df[f\"Post_{lag}_weekday_hour_mean\"]) / df[f\"Post_{lag}_weekday_hour_std\"]\n",
    "    \n",
    "    # % change between prevs & posts\n",
    "    for lag in [3]:\n",
    "        df[f\"PDiff_Prev_1_{lag}\"] = (df[f\"Prev_{lag}_{speed_feature}\"] - df[f\"Prev_1_{speed_feature}\"]) / df[f\"Prev_{lag}_{speed_feature}\"]\n",
    "        \n",
    "\n",
    "    # Prev/Post Moving simple statistics\n",
    "    for win in [3, 5, 10]:\n",
    "        df[f\"Prev_{win}_Avg_{speed_feature}\"] = df[speed_feature].shift().rolling(win, win_type=None, min_periods=1).mean()\n",
    "        df[f\"Prev_{win}_Max_{speed_feature}\"] = df[speed_feature].shift().rolling(win, win_type=None, min_periods=1).max()\n",
    "        df[f\"Prev_{win}_Min_{speed_feature}\"] = df[speed_feature].shift().rolling(win, win_type=None, min_periods=1).min()\n",
    "        \n",
    "        df[f\"Post_{win}_Avg_{speed_feature}\"] = df[f\"Prev_{win}_Avg_{speed_feature}\"].shift(-win-1)\n",
    "        df[f\"Post_{win}_Max_{speed_feature}\"] = df[f\"Prev_{win}_Max_{speed_feature}\"].shift(-win-1)\n",
    "        df[f\"Post_{win}_Min_{speed_feature}\"] = df[f\"Prev_{win}_Min_{speed_feature}\"].shift(-win-1)\n",
    "    \n",
    "        df[f\"Ratio_Prev_2_{win}_Avg\"] = df[f\"Prev_2_{speed_feature}\"] / (df[f\"Prev_{win}_Avg_{speed_feature}\"])\n",
    "        \n",
    "        df[f\"Prev_{win}_EMA_{speed_feature}\"] = df[speed_feature].shift().ewm(win, min_periods=1, ignore_na=True).mean()\n",
    "    \n",
    "    # Prev/Post Moving groupby statistics\n",
    "#     for win in [3, 5]:\n",
    "#         df[f\"Prev_{win}_Avg_hourly_{speed_feature}\"] = full_df.groupby(\"weekday_hour\")[speed_feature].shift().rolling(win, min_periods=1).median()\n",
    "# #         df[f\"Post_{win}_Avg_hourly_{speed_feature}\"] = df[f\"Prev_{win}_Avg_hourly_{speed_feature}\"].shift(-win-1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def feature_engineering(full_df, fill_speed_func):\n",
    "    full_df[\"month\"] = full_df.date.dt.month\n",
    "    full_df[\"day\"] = full_df.date.dt.day\n",
    "    full_df[\"hour\"] = full_df.date.dt.hour # 24\n",
    "    full_df[\"weekday\"] = full_df.date.dt.weekday # 7\n",
    "    full_df[\"dayofyear\"] = full_df.date.dt.dayofyear\n",
    "    full_df[\"weekofyear\"] = full_df.date.dt.weekofyear\n",
    "\n",
    "    full_df[\"holiday\"] = full_df.date.dt.floor('d').isin(holidays)\n",
    "\n",
    "    # sun -> 6 \n",
    "    full_df.loc[full_df[\"weekday\"]==0, \"weekday\"] = 7\n",
    "    full_df[\"weekday\"] -= 1\n",
    "\n",
    "    full_df[\"weekday_hour\"] = full_df[\"weekday\"] * 24 + full_df[\"hour\"]\n",
    "    full_df[\"day_weekday_hour\"] = (full_df[\"day\"]//7) * (7*24) + full_df[\"weekday\"] * 24 + full_df[\"hour\"]\n",
    "    full_df[\"quarter\"] = full_df.date.dt.quarter\n",
    "    full_df[\"year_weekday_hour\"] = full_df.date.dt.year + full_df[\"weekday\"] * 24 + full_df[\"hour\"]\n",
    "\n",
    "    full_df = full_df.sort_values(\"date\").reset_index(drop=True)\n",
    "    \n",
    "    full_df[\"hour_avg_speed\"] = full_df.groupby(\"hour\")[\"truth\"].transform(\"mean\")\n",
    "    full_df[\"hour_std_speed\"] = full_df.groupby(\"hour\")[\"truth\"].transform(\"std\")\n",
    "\n",
    "    full_df[\"weekday_hour_avg_speed\"] = full_df.groupby([\"weekday_hour\"])[\"truth\"].transform(\"mean\")\n",
    "    full_df[\"weekday_hour_std_speed\"] = full_df.groupby([\"weekday_hour\"])[\"truth\"].transform(\"std\")\n",
    "\n",
    "    full_df[\"per_change\"] = full_df[\"speed\"].pct_change()\n",
    "    \n",
    "    full_df = fill_speed_func(full_df)\n",
    "\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T05:32:38.146116Z",
     "iopub.status.busy": "2020-12-05T05:32:38.145329Z",
     "iopub.status.idle": "2020-12-05T05:32:38.148542Z",
     "shell.execute_reply": "2020-12-05T05:32:38.147896Z"
    },
    "papermill": {
     "duration": 0.041208,
     "end_time": "2020-12-05T05:32:38.148672",
     "exception": false,
     "start_time": "2020-12-05T05:32:38.107464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def prepare(full_df, fill_speed_func):\n",
    "\n",
    "    \n",
    "    full_df = feature_engineering(full_df, fill_speed_func)\n",
    "    train_df = full_df[~full_df.speed.isna()].reset_index(drop=True)\n",
    "    valid_df_2017 = full_df[(full_df.speed.isna()) & (full_df.date.dt.year==2017)].reset_index(drop=True)\n",
    "    valid_df_2018 = full_df[(~full_df.speed.isna()) & (full_df.date.dt.year==2018)].reset_index(drop=True)\n",
    "    test_df = full_df[(full_df.speed.isna()) & (full_df.date.dt.year==2018)].reset_index(drop=True)\n",
    "    \n",
    "    print(train_df.shape, valid_df_2017.shape, valid_df_2018.shape)\n",
    "    \n",
    "    selected_features = [col for col in train_df.columns if col not in [\n",
    "        \"id\", \"date\", \"per_change\", \"daily_max\", \"daily_min\",\n",
    "        \"day_weekday_hour\",\n",
    "        \"date_only\",\n",
    "        \"year_weekday_hour\",\n",
    "        \"speed_fill_median_h\",\n",
    "        \"speed_fill_median_wh\",\n",
    "        \"speed_fill_mean_h\",\n",
    "        \"speed_fill_mean_wh\",\n",
    "        \"speed_fill_mean\",\n",
    "        \"speed_fill_median\", \n",
    "        \"speed_fill_wh_median\",\n",
    "        \"lin_speed\", \"quadratic_speed\", \"nearest_speed\", \"speed\", \"is_test\", \"truth\"\n",
    "    ]]\n",
    "    \n",
    "    # fill nans\n",
    "    for col in selected_features:\n",
    "        avg = train_df[col].mean()\n",
    "        for df in [train_df, test_df, valid_df_2017, valid_df_2018]:\n",
    "            df[col] = df[col].fillna(avg)\n",
    "    \n",
    "    return train_df, valid_df_2017, valid_df_2018, test_df, selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T05:32:38.247299Z",
     "iopub.status.busy": "2020-12-05T05:32:38.228139Z",
     "iopub.status.idle": "2020-12-05T05:32:38.250504Z",
     "shell.execute_reply": "2020-12-05T05:32:38.249858Z"
    },
    "papermill": {
     "duration": 0.079796,
     "end_time": "2020-12-05T05:32:38.250634",
     "exception": false,
     "start_time": "2020-12-05T05:32:38.170838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(model_builder: Callable,\n",
    "          train_df, \n",
    "          valid_df_2017,\n",
    "          valid_df_2018,\n",
    "          test_df,\n",
    "          features: List[str],\n",
    "          scaling=True,\n",
    "          one_hot_features=[]):\n",
    "    all_train_preds = np.array([])\n",
    "    all_train_y = np.array([])\n",
    "    \n",
    "    all_valid_preds_2017 = np.array([])\n",
    "    all_valid_y_2017 = np.array([])\n",
    "    all_valid_preds_2018 = np.array([])\n",
    "    all_valid_y_2018 = np.array([])\n",
    "    \n",
    "    all_test_preds = 0\n",
    "    \n",
    "    models = []\n",
    "    num_fold = 1\n",
    "    feature_importance = None\n",
    "    \n",
    "    for fold in range(num_fold):\n",
    "        _train_df = train_df.copy()\n",
    "        _valid_df_2017 = valid_df_2017.copy()\n",
    "        _valid_df_2018 = valid_df_2018.copy()\n",
    "        \n",
    "        _features = features.copy()\n",
    "        _features = [f for f in _features if f not in one_hot_features]\n",
    "        \n",
    "        train_ohe = {}\n",
    "        valid_2017_ohe = {}\n",
    "        valid_2018_ohe = {}\n",
    "        test_ohe = {}\n",
    "        for f in one_hot_features:\n",
    "            labeler = LabelEncoder()\n",
    "            labeler.fit(_train_df[f])\n",
    "            \n",
    "            train_ohe[f] = tf.keras.utils.to_categorical(labeler.transform(_train_df[f]))\n",
    "            valid_2017_ohe[f] = tf.keras.utils.to_categorical(labeler.transform(_valid_df_2017[f]))\n",
    "            valid_2018_ohe[f] = tf.keras.utils.to_categorical(labeler.transform(_valid_df_2018[f]))\n",
    "            test_ohe[f] = tf.keras.utils.to_categorical(labeler.transform(test_df[f]))\n",
    "            \n",
    "        _train_x = _train_df[_features].values\n",
    "        _valid_x_2017 = _valid_df_2017[_features].values\n",
    "        _valid_x_2018 = _valid_df_2018[_features].values\n",
    "        _test_x = test_df[_features].copy().values\n",
    "        \n",
    "        if scaling:\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(_train_x)\n",
    "            \n",
    "            _train_x = scaler.transform(_train_x)\n",
    "            _valid_x_2017 = scaler.transform(_valid_x_2017)\n",
    "            _valid_x_2018 = scaler.transform(_valid_x_2018)\n",
    "            _test_x = scaler.transform(_test_x)\n",
    "        \n",
    "        for ohe_f in train_ohe.keys():\n",
    "            _train_x = np.concatenate([_train_x, train_ohe[ohe_f]], axis=1)\n",
    "            _valid_x_2017 = np.concatenate([_valid_x_2017, valid_2017_ohe[ohe_f]], axis=1)\n",
    "            _valid_x_2018 = np.concatenate([_valid_x_2018, valid_2018_ohe[ohe_f]], axis=1)\n",
    "            _test_x = np.concatenate([_test_x, test_ohe[ohe_f]], axis=1)\n",
    "        \n",
    "        _train_y = _train_df.speed.values\n",
    "        _valid_y_2017 = _valid_df_2017.truth.values\n",
    "        _valid_y_2018 = _valid_df_2018.truth.values            \n",
    "        \n",
    "        model = model_builder()\n",
    "        num_seed = 5\n",
    "        if isinstance(model, tf.keras.Model):\n",
    "            for seed in range(0, num_seed):\n",
    "                model = model_builder()\n",
    "                model.compile(\"adam\", loss=\"mse\")\n",
    "                history = model.fit(\n",
    "                    _train_x,\n",
    "                    _train_y,\n",
    "                    validation_data=(\n",
    "                        _valid_x_2017, _valid_y_2017\n",
    "                    ),\n",
    "                    batch_size=200, epochs=100,\n",
    "                    verbose=0,\n",
    "                    callbacks=[\n",
    "                        CosineAnnealingLearningRateSchedule(100, 10, 0.001),\n",
    "                        tf.keras.callbacks.ModelCheckpoint(f'model_{fold}_{seed}.h5', save_best_only=True, verbose=0, monitor='val_loss'),\n",
    "                    ]\n",
    "                )\n",
    "                print(min(history.history[\"val_loss\"]))\n",
    "        elif isinstance(model, lgb.LGBMRegressor):\n",
    "            sample_weights = [1 if 20 <= h <= 23 else 1 for h in train_df.hour]\n",
    "            model.fit(\n",
    "                _train_x,\n",
    "                _train_y,\n",
    "                eval_set=(\n",
    "                    _valid_x_2017, _valid_y_2017\n",
    "                ),\n",
    "                early_stopping_rounds=100,\n",
    "                verbose=False,\n",
    "                sample_weight=sample_weights,\n",
    "            )\n",
    "            \n",
    "            feature_importance = model.feature_importances_\n",
    "        elif isinstance(model, ctb.CatBoostRegressor):\n",
    "            model.fit(\n",
    "                _train_x,\n",
    "                _train_y,\n",
    "                eval_set=(\n",
    "                    _valid_x_2017, _valid_y_2017\n",
    "                ),\n",
    "                early_stopping_rounds=100,\n",
    "                verbose=False,\n",
    "            ) \n",
    "        else:\n",
    "            model.fit(\n",
    "                _train_x,\n",
    "                _train_y,\n",
    "            )\n",
    "        models.append(model)\n",
    "        \n",
    "        train_preds = 0\n",
    "        valid_preds_2017 = 0\n",
    "        valid_preds_2018 = 0\n",
    "        test_preds = 0       \n",
    "        if isinstance(model, tf.keras.Model):\n",
    "            for seed in range(num_seed):\n",
    "                model.load_weights(f'model_{fold}_{seed}.h5')\n",
    "                train_preds += model.predict(_train_x).reshape(-1) / num_seed\n",
    "                valid_preds_2017 += model.predict(_valid_x_2017).reshape(-1) / num_seed\n",
    "                valid_preds_2018 += model.predict(_valid_x_2018).reshape(-1) / num_seed\n",
    "                test_preds += model.predict(_test_x).reshape(-1) / num_seed\n",
    "        \n",
    "        else:\n",
    "            train_preds = model.predict(_train_x).reshape(-1)\n",
    "            valid_preds_2017 = model.predict(_valid_x_2017).reshape(-1)\n",
    "            valid_preds_2018 = model.predict(_valid_x_2018).reshape(-1)\n",
    "            test_preds = model.predict(_test_x).reshape(-1)\n",
    "        all_test_preds += test_preds / num_fold\n",
    "\n",
    "\n",
    "        train_mse = mean_squared_error(_train_y, train_preds)\n",
    "        valid_mse_2017 = mean_squared_error(_valid_y_2017, valid_preds_2017)\n",
    "        valid_mse_2018 = mean_squared_error(_valid_y_2018, valid_preds_2018)\n",
    "#         print(train_mse, valid_mse_2017, valid_mse_2018)\n",
    "\n",
    "#         print(train_preds.shape)\n",
    "        all_train_preds = np.concatenate([all_train_preds, train_preds]) \n",
    "        all_train_y = np.concatenate([all_train_y, _train_y])\n",
    "\n",
    "        all_valid_preds_2017 = np.concatenate([all_valid_preds_2017, valid_preds_2017]) \n",
    "        all_valid_y_2017 = np.concatenate([all_valid_y_2017, _valid_y_2017]) \n",
    "\n",
    "        all_valid_preds_2018 = np.concatenate([all_valid_preds_2018, valid_preds_2018]) \n",
    "        all_valid_y_2018 = np.concatenate([all_valid_y_2018, _valid_y_2018]) \n",
    "    \n",
    "    train_mse = mean_squared_error(all_train_y, all_train_preds)\n",
    "    valid_mse_2017 = mean_squared_error(all_valid_y_2017, all_valid_preds_2017)\n",
    "    valid_mse_2018 = mean_squared_error(all_valid_y_2018, all_valid_preds_2018)\n",
    "    \n",
    "    print(train_mse, valid_mse_2017, valid_mse_2018)\n",
    "    return {\n",
    "        \"models\": models,\n",
    "        \"all_train_preds\": all_train_preds,\n",
    "        \"all_train_y\": all_train_y,\n",
    "        \n",
    "        \"all_valid_preds_2017\": all_valid_preds_2017,\n",
    "        \"all_valid_y_2017\": all_valid_y_2017,\n",
    "        \"all_valid_preds_2018\": all_valid_preds_2018,\n",
    "        \"all_valid_y_2018\": all_valid_y_2018,\n",
    "        \"test_preds\": all_test_preds,\n",
    "        \"features\": _features,\n",
    "        \"feature_importance\": feature_importance, \n",
    "        \"valid_mse_2017\": valid_mse_2017,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T05:32:38.331531Z",
     "iopub.status.busy": "2020-12-05T05:32:38.303912Z",
     "iopub.status.idle": "2020-12-05T05:32:38.334681Z",
     "shell.execute_reply": "2020-12-05T05:32:38.333987Z"
    },
    "papermill": {
     "duration": 0.062014,
     "end_time": "2020-12-05T05:32:38.334808",
     "exception": false,
     "start_time": "2020-12-05T05:32:38.272794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_models(train_df, valid_df_2017, valid_df_2018, test_df, selected_features):\n",
    "    init = tf.keras.initializers.LecunNormal()\n",
    "    keras_one_hot_features = [\"hour\", \"weekday\"]\n",
    "    \n",
    "    model_builders = {\n",
    "        \"lgb\": lambda :lgb.LGBMRegressor(\n",
    "                boosting_type='gbdt', \n",
    "                objective=\"mse\",\n",
    "                min_child_samples=20,\n",
    "                num_leaves=40,\n",
    "                max_depth=16,\n",
    "                learning_rate=0.05,\n",
    "                n_estimators=1000,\n",
    "                colsample_bytree=.4,\n",
    "                subsample=1.0,\n",
    "                subsample_freq=0,\n",
    "                importance_type=\"gain\",\n",
    "                random_state=random_state,\n",
    "                n_jobs=-1,\n",
    "        ),\n",
    "        'lgbrf': lambda :lgb.LGBMRegressor(\n",
    "                boosting_type='rf', \n",
    "                objective=\"mse\",\n",
    "                min_child_samples=20,\n",
    "                num_leaves=128,\n",
    "                max_depth=16,\n",
    "                learning_rate=0.05,\n",
    "                n_estimators=1000,\n",
    "                colsample_bytree=.5,\n",
    "                subsample=.95,\n",
    "                subsample_freq=20,\n",
    "                importance_type=\"gain\",\n",
    "                random_state=random_state,\n",
    "                n_jobs=-1,\n",
    "        ),\n",
    "        'cat': lambda :ctb.CatBoostRegressor(\n",
    "                loss_function=\"RMSE\",\n",
    "                learning_rate=.06,\n",
    "                max_depth=8,\n",
    "                min_child_samples=20,\n",
    "                colsample_bylevel=1.0,\n",
    "                n_estimators=None,\n",
    "                use_best_model=True,\n",
    "                random_seed=random_state,\n",
    "        ),\n",
    "        'keras': lambda :tf.keras.Sequential(\n",
    "                    layers=[\n",
    "                        tf.keras.Input(shape=(len(selected_features) - len(keras_one_hot_features) + sum([full_df[f].nunique() for f in keras_one_hot_features]),)),  \n",
    "\n",
    "                        tf.keras.layers.Dense(20, kernel_initializer=init, activation=tfa.activations.mish),\n",
    "                        tf.keras.layers.Dropout(.05),\n",
    "                        tf.keras.layers.BatchNormalization(),\n",
    "                        tf.keras.layers.GaussianNoise(.15),\n",
    "\n",
    "                        tf.keras.layers.Dense(20, kernel_initializer=init, activation=\"tanh\"),\n",
    "                        tf.keras.layers.Dropout(.05),\n",
    "                        tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "                        tf.keras.layers.Dense(1, kernel_initializer=init, activation=\"linear\"),\n",
    "                    ]\n",
    "        ),\n",
    "        'lr': lambda :LinearRegression(),\n",
    "        'rr': lambda :Ridge(),\n",
    "        'sdg': lambda :SGDRegressor(max_iter=800),\n",
    "        'etr': lambda :ExtraTreesRegressor(\n",
    "                    n_estimators=400,\n",
    "                    max_depth=18,\n",
    "                    max_leaf_nodes=None,\n",
    "                    criterion=\"mse\",\n",
    "                    min_samples_split=25,\n",
    "                    max_features=.9,\n",
    "                    bootstrap=False,\n",
    "                    random_state=random_state,\n",
    "                    n_jobs=-1,\n",
    "        ),\n",
    "        'hgbr': lambda :HistGradientBoostingRegressor(\n",
    "                    max_iter=200,\n",
    "                    max_leaf_nodes=64,\n",
    "                    max_depth=16,\n",
    "                    learning_rate=.04,\n",
    "                    random_state=random_state,\n",
    "        ),\n",
    "        'svr': lambda :SVR(C=300),\n",
    "        'mlp': lambda :\n",
    "                MLPRegressor(\n",
    "                    hidden_layer_sizes=(16, 16), activation='tanh', solver='adam', \n",
    "                    learning_rate=\"adaptive\", shuffle=True,\n",
    "                    max_iter=120, random_state=random_state,\n",
    "        ),\n",
    "        'gbr': lambda :GradientBoostingRegressor(\n",
    "            subsample=.8,\n",
    "            n_estimators=120,\n",
    "            min_samples_leaf=30, \n",
    "            max_depth=8,\n",
    "            learning_rate=.04,\n",
    "            criterion=\"mse\",\n",
    "            min_samples_split=20,\n",
    "            random_state=random_state,\n",
    "        ),\n",
    "        'rf': lambda :RandomForestRegressor(\n",
    "            n_estimators=400,\n",
    "            max_depth=12,\n",
    "            min_samples_leaf=6,\n",
    "            max_features=.4,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "        ),\n",
    "        'knn': lambda :KNeighborsRegressor(\n",
    "            n_neighbors=20,\n",
    "            weights=\"distance\",\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    print(\"#\"*5, \"Models\", \"#\"*5)\n",
    "    model_results = {}\n",
    "    for key, model_builder in model_builders.items():\n",
    "        scaling = False\n",
    "        one_hot_features = []\n",
    "        \n",
    "        if key == 'keras':\n",
    "            one_hot_features = keras_one_hot_features\n",
    "        \n",
    "        if key in ['keras', 'lr', 'rr', 'sdg', 'mlp', 'rf', 'knn']:\n",
    "            scaling = True\n",
    "        \n",
    "        print(key)\n",
    "        result = train(\n",
    "            model_builder,\n",
    "            train_df,\n",
    "            valid_df_2017,\n",
    "            valid_df_2018,\n",
    "            test_df,\n",
    "            selected_features,\n",
    "            scaling=scaling,\n",
    "            one_hot_features=one_hot_features,\n",
    "        )\n",
    "        model_results[key] = result\n",
    "    return model_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T05:32:38.400512Z",
     "iopub.status.busy": "2020-12-05T05:32:38.399177Z",
     "iopub.status.idle": "2020-12-05T05:32:38.403911Z",
     "shell.execute_reply": "2020-12-05T05:32:38.403231Z"
    },
    "papermill": {
     "duration": 0.046641,
     "end_time": "2020-12-05T05:32:38.404044",
     "exception": false,
     "start_time": "2020-12-05T05:32:38.357403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def blending(model_results):\n",
    "    train_preds_df = pd.DataFrame({\n",
    "        k: r[\"all_train_preds\"] for k, r in model_results.items()\n",
    "    })\n",
    "    train_preds_df[\"y\"] = model_results[list(model_results.keys())[0]][\"all_train_y\"]\n",
    "    \n",
    "    \n",
    "    oof_df_2017 = pd.DataFrame({\n",
    "        k: r[\"all_valid_preds_2017\"] for k, r in model_results.items()\n",
    "    })\n",
    "    oof_df_2017[\"y\"] = model_results[list(model_results.keys())[0]][\"all_valid_y_2017\"]\n",
    "\n",
    "    \n",
    "    oof_cols = [col for col in oof_df_2017.columns if col not in [\"y\"]]\n",
    "    \n",
    "    blending_lr = LinearRegression()\n",
    "\n",
    "    # train blender with 2017 data\n",
    "    blending_lr.fit(\n",
    "        oof_df_2017[oof_cols].values,\n",
    "        oof_df_2017[\"y\"].values,\n",
    "    )\n",
    "    \n",
    "\n",
    "#     display(pd.DataFrame({\n",
    "#         \"model\": oof_cols,\n",
    "#         \"weight\": blending_lr.coef_\n",
    "#     }).sort_values(\"weight\"))\n",
    "\n",
    "    # blend valid\n",
    "    oof_df_2017[\"blended\"] =  0\n",
    "    for i in range(len(oof_cols)):\n",
    "        oof_df_2017[\"blended\"] += oof_df_2017[oof_cols[i]] * blending_lr.coef_[i]\n",
    "    oof_df_2017[\"blended\"] += blending_lr.intercept_\n",
    "    \n",
    "    # blend train\n",
    "    train_preds_df[\"blended\"] =  0\n",
    "    for i in range(len(oof_cols)):\n",
    "        train_preds_df[\"blended\"] += train_preds_df[oof_cols[i]] * blending_lr.coef_[i]\n",
    "    train_preds_df[\"blended\"] += blending_lr.intercept_\n",
    "\n",
    "    \n",
    "    print(\"Blending CV\")\n",
    "    mse_2017 = mean_squared_error(oof_df_2017[\"y\"], oof_df_2017[\"blended\"])\n",
    "    print(mean_squared_error(train_preds_df[\"y\"], train_preds_df[\"blended\"]), mse_2017)\n",
    "    \n",
    "    # blend submissions\n",
    "    sub_df = pd.DataFrame({\n",
    "        k: r[\"test_preds\"] for k, r in model_results.items()\n",
    "    })\n",
    "    \n",
    "    sub_df[\"blended\"] =  0\n",
    "    for i in range(len(oof_cols)):\n",
    "        sub_df[\"blended\"] += sub_df[oof_cols[i]] * blending_lr.coef_[i]\n",
    "    sub_df[\"blended\"] += blending_lr.intercept_\n",
    "\n",
    "    test_df[\"speed\"] = sub_df[\"blended\"].values\n",
    "#     test_df[[\"id\", \"speed\"]].to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "    return train_preds_df, oof_df_2017, test_df[[\"id\", \"speed\"]], mse_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T05:32:38.455699Z",
     "iopub.status.busy": "2020-12-05T05:32:38.454694Z",
     "iopub.status.idle": "2020-12-05T05:32:38.457762Z",
     "shell.execute_reply": "2020-12-05T05:32:38.457146Z"
    },
    "papermill": {
     "duration": 0.030173,
     "end_time": "2020-12-05T05:32:38.457895",
     "exception": false,
     "start_time": "2020-12-05T05:32:38.427722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022456,
     "end_time": "2020-12-05T05:32:38.503155",
     "exception": false,
     "start_time": "2020-12-05T05:32:38.480699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T05:32:38.567596Z",
     "iopub.status.busy": "2020-12-05T05:32:38.566748Z",
     "iopub.status.idle": "2020-12-05T05:38:06.935521Z",
     "shell.execute_reply": "2020-12-05T05:38:06.934886Z"
    },
    "papermill": {
     "duration": 328.408454,
     "end_time": "2020-12-05T05:38:06.935658",
     "exception": false,
     "start_time": "2020-12-05T05:32:38.527204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing intervals 2018\n",
      "0 days 01:00:00    3135\n",
      "0 days 02:00:00    1260\n",
      "0 days 03:00:00     526\n",
      "0 days 04:00:00     217\n",
      "0 days 05:00:00      75\n",
      "0 days 06:00:00      30\n",
      "0 days 07:00:00       8\n",
      "0 days 10:00:00       2\n",
      "0 days 08:00:00       2\n",
      "0 days 09:00:00       1\n",
      "Name: interval, dtype: int64\n",
      "2      2018-01-01 02:00:00+08:00\n",
      "5      2018-01-01 05:00:00+08:00\n",
      "7      2018-01-01 07:00:00+08:00\n",
      "8      2018-01-01 08:00:00+08:00\n",
      "10     2018-01-01 10:00:00+08:00\n",
      "                  ...           \n",
      "8753   2018-12-31 17:00:00+08:00\n",
      "8755   2018-12-31 19:00:00+08:00\n",
      "8757   2018-12-31 21:00:00+08:00\n",
      "8758   2018-12-31 22:00:00+08:00\n",
      "8759   2018-12-31 23:00:00+08:00\n",
      "Name: date, Length: 3504, dtype: datetime64[ns, Hongkong]\n",
      "2      2017-01-01 02:00:00+08:00\n",
      "5      2017-01-01 05:00:00+08:00\n",
      "7      2017-01-01 07:00:00+08:00\n",
      "8      2017-01-01 08:00:00+08:00\n",
      "10     2017-01-01 10:00:00+08:00\n",
      "                  ...           \n",
      "8753   2017-12-31 17:00:00+08:00\n",
      "8755   2017-12-31 19:00:00+08:00\n",
      "8757   2017-12-31 21:00:00+08:00\n",
      "8758   2017-12-31 22:00:00+08:00\n",
      "8759   2017-12-31 23:00:00+08:00\n",
      "Name: date, Length: 3504, dtype: datetime64[ns, Hongkong]\n",
      "(10509, 98) (3497, 98) (5256, 98)\n",
      "##### Models #####\n",
      "lgb\n",
      "6.844240870610412 9.526547771307529 6.9480764585806\n",
      "lgbrf\n",
      "8.333639555216294 9.836052298068438 8.506445171132361\n",
      "cat\n",
      "6.061930406433417 9.559015780036361 6.076409636758995\n",
      "keras\n",
      "9.613815307617188\n",
      "9.640220642089844\n",
      "9.700165748596191\n",
      "9.581018447875977\n",
      "9.56815242767334\n",
      "8.833051936151193 9.369455407643951 8.991022700772582\n",
      "lr\n",
      "11.24070831485388 10.410097216847623 11.361040489211904\n",
      "rr\n",
      "11.12468018156961 10.302641715420375 11.26122407025454\n",
      "sdg\n",
      "11.305721443400234 10.424967607025442 11.398162398612133\n",
      "etr\n",
      "4.426988328357204 9.647776761762838 4.4387570850036875\n",
      "hgbr\n",
      "5.424950208767479 9.504139592476998 5.537005622086705\n",
      "svr\n",
      "10.332443809985316 10.325997313130818 10.339637908339208\n",
      "mlp\n",
      "9.159941172200016 10.575261082883491 9.159650994750633\n",
      "gbr\n",
      "6.598179101945332 9.37733453969807 6.732136270926095\n",
      "rf\n",
      "6.0840839347314155 9.511657583316579 6.213211220246654\n",
      "knn\n",
      "0.0 11.773999275975948 0.0\n",
      "Blending CV\n",
      "7.147750975471089 9.12377280034462\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0af672338ea9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mblended_train_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblended_val_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblended_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_2017\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblending\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mblended_sub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"speed\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"submission_{i}.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmse_2017\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "def fill_speed_func(df):\n",
    "    df[\"speed_fill_median\"] = df.groupby(\"weekday_hour\")[\"speed\"].transform(\"median\")\n",
    "    df.loc[~full_df.speed.isna(), \"speed_fill_median\"] = df.loc[~df.speed.isna(), \"speed\"]\n",
    "    df = speed_feature_engineering(df, 'speed_fill_median')\n",
    "    return df\n",
    "\n",
    "subs = []\n",
    "sample_submission_df, train_df, test_df, all_df = read_files()\n",
    "train_df, test_df, full_df, full_2017_df, full_2018_df = handle_missing_dates(\n",
    "    train_df, test_df, all_df, hole_random_state=2020)\n",
    "train_df, valid_df_2017, valid_df_2018, test_df, selected_features = prepare(full_df, fill_speed_func)\n",
    "model_results = train_models(train_df, valid_df_2017, valid_df_2018, test_df, selected_features)\n",
    "\n",
    "blended_train_preds, blended_val_preds, blended_sub, mse_2017 = blending(model_results)\n",
    "blended_sub[[\"id\", \"speed\"]].to_csv(f\"submission_{i}.csv\", index=False)\n",
    "\n",
    "mse_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T05:38:07.006696Z",
     "iopub.status.busy": "2020-12-05T05:38:07.005690Z",
     "iopub.status.idle": "2020-12-05T05:38:07.008959Z",
     "shell.execute_reply": "2020-12-05T05:38:07.008327Z"
    },
    "papermill": {
     "duration": 0.040619,
     "end_time": "2020-12-05T05:38:07.009090",
     "exception": false,
     "start_time": "2020-12-05T05:38:06.968471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stage_1_model_results = model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T05:38:07.083962Z",
     "iopub.status.busy": "2020-12-05T05:38:07.082778Z",
     "iopub.status.idle": "2020-12-05T05:38:07.087172Z",
     "shell.execute_reply": "2020-12-05T05:38:07.087777Z"
    },
    "papermill": {
     "duration": 0.046291,
     "end_time": "2020-12-05T05:38:07.087934",
     "exception": false,
     "start_time": "2020-12-05T05:38:07.041643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['month',\n",
       " 'day',\n",
       " 'hour',\n",
       " 'weekday',\n",
       " 'dayofyear',\n",
       " 'weekofyear',\n",
       " 'holiday',\n",
       " 'weekday_hour',\n",
       " 'quarter',\n",
       " 'hour_avg_speed',\n",
       " 'hour_std_speed',\n",
       " 'weekday_hour_avg_speed',\n",
       " 'weekday_hour_std_speed',\n",
       " 'Prev_1_%_change',\n",
       " 'Prev_2_%_change',\n",
       " 'Prev_3_%_change',\n",
       " 'Prev_1_speed_fill_median',\n",
       " 'Post_1_speed_fill_median',\n",
       " 'Prev_1_weekday_hour_mean',\n",
       " 'Post_1_weekday_hour_mean',\n",
       " 'Prev_1_weekday_hour_std',\n",
       " 'Post_1_weekday_hour_std',\n",
       " 'Ratio_Prev_1_weekday_hour_mean',\n",
       " 'Ratio_Post_1_weekday_hour_mean',\n",
       " 'PDiff_Prev_1_weekday_hour_mean',\n",
       " 'PDiff_Post_1_weekday_hour_mean',\n",
       " 'Z_Score_Prev_1_weekday_hour',\n",
       " 'Z_Score_Post_1_weekday_hour',\n",
       " 'Prev_2_speed_fill_median',\n",
       " 'Post_2_speed_fill_median',\n",
       " 'Prev_2_weekday_hour_mean',\n",
       " 'Post_2_weekday_hour_mean',\n",
       " 'Prev_2_weekday_hour_std',\n",
       " 'Post_2_weekday_hour_std',\n",
       " 'Ratio_Prev_2_weekday_hour_mean',\n",
       " 'Ratio_Post_2_weekday_hour_mean',\n",
       " 'PDiff_Prev_2_weekday_hour_mean',\n",
       " 'PDiff_Post_2_weekday_hour_mean',\n",
       " 'Z_Score_Prev_2_weekday_hour',\n",
       " 'Z_Score_Post_2_weekday_hour',\n",
       " 'Prev_3_speed_fill_median',\n",
       " 'Post_3_speed_fill_median',\n",
       " 'Prev_3_weekday_hour_mean',\n",
       " 'Post_3_weekday_hour_mean',\n",
       " 'Prev_3_weekday_hour_std',\n",
       " 'Post_3_weekday_hour_std',\n",
       " 'Ratio_Prev_3_weekday_hour_mean',\n",
       " 'Ratio_Post_3_weekday_hour_mean',\n",
       " 'PDiff_Prev_3_weekday_hour_mean',\n",
       " 'PDiff_Post_3_weekday_hour_mean',\n",
       " 'Z_Score_Prev_3_weekday_hour',\n",
       " 'Z_Score_Post_3_weekday_hour',\n",
       " 'Prev_24_speed_fill_median',\n",
       " 'Post_24_speed_fill_median',\n",
       " 'Prev_24_weekday_hour_mean',\n",
       " 'Post_24_weekday_hour_mean',\n",
       " 'Prev_24_weekday_hour_std',\n",
       " 'Post_24_weekday_hour_std',\n",
       " 'Ratio_Prev_24_weekday_hour_mean',\n",
       " 'Ratio_Post_24_weekday_hour_mean',\n",
       " 'PDiff_Prev_24_weekday_hour_mean',\n",
       " 'PDiff_Post_24_weekday_hour_mean',\n",
       " 'Z_Score_Prev_24_weekday_hour',\n",
       " 'Z_Score_Post_24_weekday_hour',\n",
       " 'PDiff_Prev_1_3',\n",
       " 'Prev_3_Avg_speed_fill_median',\n",
       " 'Prev_3_Max_speed_fill_median',\n",
       " 'Prev_3_Min_speed_fill_median',\n",
       " 'Post_3_Avg_speed_fill_median',\n",
       " 'Post_3_Max_speed_fill_median',\n",
       " 'Post_3_Min_speed_fill_median',\n",
       " 'Ratio_Prev_2_3_Avg',\n",
       " 'Prev_3_EMA_speed_fill_median',\n",
       " 'Prev_5_Avg_speed_fill_median',\n",
       " 'Prev_5_Max_speed_fill_median',\n",
       " 'Prev_5_Min_speed_fill_median',\n",
       " 'Post_5_Avg_speed_fill_median',\n",
       " 'Post_5_Max_speed_fill_median',\n",
       " 'Post_5_Min_speed_fill_median',\n",
       " 'Ratio_Prev_2_5_Avg',\n",
       " 'Prev_5_EMA_speed_fill_median',\n",
       " 'Prev_10_Avg_speed_fill_median',\n",
       " 'Prev_10_Max_speed_fill_median',\n",
       " 'Prev_10_Min_speed_fill_median',\n",
       " 'Post_10_Avg_speed_fill_median',\n",
       " 'Post_10_Max_speed_fill_median',\n",
       " 'Post_10_Min_speed_fill_median',\n",
       " 'Ratio_Prev_2_10_Avg',\n",
       " 'Prev_10_EMA_speed_fill_median']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033689,
     "end_time": "2020-12-05T05:38:07.155281",
     "exception": false,
     "start_time": "2020-12-05T05:38:07.121592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T05:38:07.241027Z",
     "iopub.status.busy": "2020-12-05T05:38:07.239865Z",
     "iopub.status.idle": "2020-12-05T05:43:37.940338Z",
     "shell.execute_reply": "2020-12-05T05:43:37.939502Z"
    },
    "papermill": {
     "duration": 330.751749,
     "end_time": "2020-12-05T05:43:37.940509",
     "exception": false,
     "start_time": "2020-12-05T05:38:07.188760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing intervals 2018\n",
      "0 days 01:00:00    3135\n",
      "0 days 02:00:00    1260\n",
      "0 days 03:00:00     526\n",
      "0 days 04:00:00     217\n",
      "0 days 05:00:00      75\n",
      "0 days 06:00:00      30\n",
      "0 days 07:00:00       8\n",
      "0 days 10:00:00       2\n",
      "0 days 08:00:00       2\n",
      "0 days 09:00:00       1\n",
      "Name: interval, dtype: int64\n",
      "2      2018-01-01 02:00:00+08:00\n",
      "5      2018-01-01 05:00:00+08:00\n",
      "7      2018-01-01 07:00:00+08:00\n",
      "8      2018-01-01 08:00:00+08:00\n",
      "10     2018-01-01 10:00:00+08:00\n",
      "                  ...           \n",
      "8753   2018-12-31 17:00:00+08:00\n",
      "8755   2018-12-31 19:00:00+08:00\n",
      "8757   2018-12-31 21:00:00+08:00\n",
      "8758   2018-12-31 22:00:00+08:00\n",
      "8759   2018-12-31 23:00:00+08:00\n",
      "Name: date, Length: 3504, dtype: datetime64[ns, Hongkong]\n",
      "2      2017-01-01 02:00:00+08:00\n",
      "5      2017-01-01 05:00:00+08:00\n",
      "7      2017-01-01 07:00:00+08:00\n",
      "8      2017-01-01 08:00:00+08:00\n",
      "10     2017-01-01 10:00:00+08:00\n",
      "                  ...           \n",
      "8753   2017-12-31 17:00:00+08:00\n",
      "8755   2017-12-31 19:00:00+08:00\n",
      "8757   2017-12-31 21:00:00+08:00\n",
      "8758   2017-12-31 22:00:00+08:00\n",
      "8759   2017-12-31 23:00:00+08:00\n",
      "Name: date, Length: 3504, dtype: datetime64[ns, Hongkong]\n",
      "(10509, 98) (3497, 98) (5256, 98)\n",
      "##### Models #####\n",
      "lgb\n",
      "4.674875948319928 9.632132053648817 4.709606560810151\n",
      "lgbrf\n",
      "5.8728983217492665 9.688712577930042 5.884001361098809\n",
      "cat\n",
      "5.352840018496227 9.645311259203103 5.4181544449126084\n",
      "keras\n",
      "9.643951416015625\n",
      "9.47977352142334\n",
      "9.459489822387695\n",
      "9.491569519042969\n",
      "9.564973831176758\n",
      "6.283814262636756 9.319961149385676 6.353035666003379\n",
      "lr\n",
      "8.02843145839346 9.797008236020337 8.050047190856173\n",
      "rr\n",
      "8.030208820404535 9.791347910080328 8.049230062538621\n",
      "sdg\n",
      "8.21306367014408 9.91661262925935 8.213131362140462\n",
      "etr\n",
      "2.857113493595703 9.368667691061924 2.829387821452909\n",
      "hgbr\n",
      "2.800461076448866 9.982631690638438 2.8479589547349655\n",
      "svr\n",
      "7.406912840807585 9.64813467481793 7.429344801661934\n",
      "mlp\n",
      "6.39416191820589 10.128842921000748 6.42780507647181\n",
      "gbr\n",
      "3.7195902175773843 9.704887777310416 3.7151434192318726\n",
      "rf\n",
      "3.5033734125447795 9.506579500454146 3.4934901170118255\n",
      "knn\n",
      "0.0 10.630310316744929 0.0\n",
      "Blending CV\n",
      "3.8138252701111033 8.972853882067042\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5bee31f9612c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mblended_train_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblended_val_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblended_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_2017\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblending\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mblended_sub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"speed\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"submission_{i}.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mmse_2017\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "def fill_speed_func(df):\n",
    "    df[\"speed_fill_median\"] = df[\"speed\"]\n",
    "    # 2017 holes\n",
    "    df.loc[(df.date.dt.year==2017)&(df.speed.isna()), \"speed_fill_median\"] = blended_val_preds[\"blended\"].values\n",
    "    # 2018 (test holes)\n",
    "    df.loc[(df.date.dt.year==2018)&(df.speed.isna()), \"speed_fill_median\"] = blended_sub[\"speed\"].values\n",
    "\n",
    "    df = speed_feature_engineering(df, 'speed_fill_median')\n",
    "    return df\n",
    "\n",
    "subs = []\n",
    "sample_submission_df, train_df, test_df, all_df = read_files()\n",
    "train_df, test_df, full_df, full_2017_df, full_2018_df = handle_missing_dates(\n",
    "    train_df, test_df, all_df, hole_random_state=2020)\n",
    "train_df, valid_df_2017, valid_df_2018, test_df, selected_features = prepare(full_df, fill_speed_func)\n",
    "model_results = train_models(train_df, valid_df_2017, valid_df_2018, test_df, selected_features)\n",
    "\n",
    "blended_train_preds, blended_val_preds, blended_sub, mse_2017 = blending(model_results)\n",
    "blended_sub[[\"id\", \"speed\"]].to_csv(f\"submission_{i}.csv\", index=False)\n",
    "\n",
    "mse_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T05:43:38.033054Z",
     "iopub.status.busy": "2020-12-05T05:43:38.032267Z",
     "iopub.status.idle": "2020-12-05T05:43:38.035361Z",
     "shell.execute_reply": "2020-12-05T05:43:38.035930Z"
    },
    "papermill": {
     "duration": 0.052446,
     "end_time": "2020-12-05T05:43:38.036100",
     "exception": false,
     "start_time": "2020-12-05T05:43:37.983654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stage_2_model_results = model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T05:43:38.130527Z",
     "iopub.status.busy": "2020-12-05T05:43:38.129405Z",
     "iopub.status.idle": "2020-12-05T05:43:38.134800Z",
     "shell.execute_reply": "2020-12-05T05:43:38.134060Z"
    },
    "papermill": {
     "duration": 0.055574,
     "end_time": "2020-12-05T05:43:38.134925",
     "exception": false,
     "start_time": "2020-12-05T05:43:38.079351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['month',\n",
       " 'day',\n",
       " 'hour',\n",
       " 'weekday',\n",
       " 'dayofyear',\n",
       " 'weekofyear',\n",
       " 'holiday',\n",
       " 'weekday_hour',\n",
       " 'quarter',\n",
       " 'hour_avg_speed',\n",
       " 'hour_std_speed',\n",
       " 'weekday_hour_avg_speed',\n",
       " 'weekday_hour_std_speed',\n",
       " 'Prev_1_%_change',\n",
       " 'Prev_2_%_change',\n",
       " 'Prev_3_%_change',\n",
       " 'Prev_1_speed_fill_median',\n",
       " 'Post_1_speed_fill_median',\n",
       " 'Prev_1_weekday_hour_mean',\n",
       " 'Post_1_weekday_hour_mean',\n",
       " 'Prev_1_weekday_hour_std',\n",
       " 'Post_1_weekday_hour_std',\n",
       " 'Ratio_Prev_1_weekday_hour_mean',\n",
       " 'Ratio_Post_1_weekday_hour_mean',\n",
       " 'PDiff_Prev_1_weekday_hour_mean',\n",
       " 'PDiff_Post_1_weekday_hour_mean',\n",
       " 'Z_Score_Prev_1_weekday_hour',\n",
       " 'Z_Score_Post_1_weekday_hour',\n",
       " 'Prev_2_speed_fill_median',\n",
       " 'Post_2_speed_fill_median',\n",
       " 'Prev_2_weekday_hour_mean',\n",
       " 'Post_2_weekday_hour_mean',\n",
       " 'Prev_2_weekday_hour_std',\n",
       " 'Post_2_weekday_hour_std',\n",
       " 'Ratio_Prev_2_weekday_hour_mean',\n",
       " 'Ratio_Post_2_weekday_hour_mean',\n",
       " 'PDiff_Prev_2_weekday_hour_mean',\n",
       " 'PDiff_Post_2_weekday_hour_mean',\n",
       " 'Z_Score_Prev_2_weekday_hour',\n",
       " 'Z_Score_Post_2_weekday_hour',\n",
       " 'Prev_3_speed_fill_median',\n",
       " 'Post_3_speed_fill_median',\n",
       " 'Prev_3_weekday_hour_mean',\n",
       " 'Post_3_weekday_hour_mean',\n",
       " 'Prev_3_weekday_hour_std',\n",
       " 'Post_3_weekday_hour_std',\n",
       " 'Ratio_Prev_3_weekday_hour_mean',\n",
       " 'Ratio_Post_3_weekday_hour_mean',\n",
       " 'PDiff_Prev_3_weekday_hour_mean',\n",
       " 'PDiff_Post_3_weekday_hour_mean',\n",
       " 'Z_Score_Prev_3_weekday_hour',\n",
       " 'Z_Score_Post_3_weekday_hour',\n",
       " 'Prev_24_speed_fill_median',\n",
       " 'Post_24_speed_fill_median',\n",
       " 'Prev_24_weekday_hour_mean',\n",
       " 'Post_24_weekday_hour_mean',\n",
       " 'Prev_24_weekday_hour_std',\n",
       " 'Post_24_weekday_hour_std',\n",
       " 'Ratio_Prev_24_weekday_hour_mean',\n",
       " 'Ratio_Post_24_weekday_hour_mean',\n",
       " 'PDiff_Prev_24_weekday_hour_mean',\n",
       " 'PDiff_Post_24_weekday_hour_mean',\n",
       " 'Z_Score_Prev_24_weekday_hour',\n",
       " 'Z_Score_Post_24_weekday_hour',\n",
       " 'PDiff_Prev_1_3',\n",
       " 'Prev_3_Avg_speed_fill_median',\n",
       " 'Prev_3_Max_speed_fill_median',\n",
       " 'Prev_3_Min_speed_fill_median',\n",
       " 'Post_3_Avg_speed_fill_median',\n",
       " 'Post_3_Max_speed_fill_median',\n",
       " 'Post_3_Min_speed_fill_median',\n",
       " 'Ratio_Prev_2_3_Avg',\n",
       " 'Prev_3_EMA_speed_fill_median',\n",
       " 'Prev_5_Avg_speed_fill_median',\n",
       " 'Prev_5_Max_speed_fill_median',\n",
       " 'Prev_5_Min_speed_fill_median',\n",
       " 'Post_5_Avg_speed_fill_median',\n",
       " 'Post_5_Max_speed_fill_median',\n",
       " 'Post_5_Min_speed_fill_median',\n",
       " 'Ratio_Prev_2_5_Avg',\n",
       " 'Prev_5_EMA_speed_fill_median',\n",
       " 'Prev_10_Avg_speed_fill_median',\n",
       " 'Prev_10_Max_speed_fill_median',\n",
       " 'Prev_10_Min_speed_fill_median',\n",
       " 'Post_10_Avg_speed_fill_median',\n",
       " 'Post_10_Max_speed_fill_median',\n",
       " 'Post_10_Min_speed_fill_median',\n",
       " 'Ratio_Prev_2_10_Avg',\n",
       " 'Prev_10_EMA_speed_fill_median']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.043139,
     "end_time": "2020-12-05T05:43:38.222195",
     "exception": false,
     "start_time": "2020-12-05T05:43:38.179056",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T05:43:38.319030Z",
     "iopub.status.busy": "2020-12-05T05:43:38.317930Z",
     "iopub.status.idle": "2020-12-05T05:43:38.322266Z",
     "shell.execute_reply": "2020-12-05T05:43:38.321552Z"
    },
    "papermill": {
     "duration": 0.055604,
     "end_time": "2020-12-05T05:43:38.322404",
     "exception": false,
     "start_time": "2020-12-05T05:43:38.266800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lgb_1', 'lgbrf_1', 'cat_1', 'keras_1', 'lr_1', 'rr_1', 'sdg_1', 'etr_1', 'hgbr_1', 'svr_1', 'mlp_1', 'gbr_1', 'rf_1', 'knn_1', 'lgb_2', 'lgbrf_2', 'cat_2', 'keras_2', 'lr_2', 'rr_2', 'sdg_2', 'etr_2', 'hgbr_2', 'svr_2', 'mlp_2', 'gbr_2', 'rf_2', 'knn_2'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_model_results = {}\n",
    "for key, item in stage_1_model_results.items():\n",
    "    combine_model_results[f\"{key}_1\"] = item\n",
    "for key, item in stage_2_model_results.items():\n",
    "    combine_model_results[f\"{key}_2\"] = item\n",
    "combine_model_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T05:43:38.419607Z",
     "iopub.status.busy": "2020-12-05T05:43:38.418353Z",
     "iopub.status.idle": "2020-12-05T05:43:38.804458Z",
     "shell.execute_reply": "2020-12-05T05:43:38.803696Z"
    },
    "papermill": {
     "duration": 0.438237,
     "end_time": "2020-12-05T05:43:38.804588",
     "exception": false,
     "start_time": "2020-12-05T05:43:38.366351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blending CV\n",
      "5.115833163149732 8.836303493417866\n"
     ]
    }
   ],
   "source": [
    "blended_train_preds, blended_val_preds, blended_sub, mse_2017 = blending(combine_model_results)\n",
    "blended_sub[[\"id\", \"speed\"]].to_csv(f\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.044452,
     "end_time": "2020-12-05T05:43:38.894282",
     "exception": false,
     "start_time": "2020-12-05T05:43:38.849830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 674.606987,
   "end_time": "2020-12-05T05:43:39.148696",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-05T05:32:24.541709",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
